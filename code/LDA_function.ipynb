{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "\n",
    "def LDA_preprocessing(data, n_documents, test_size, min_word_count):\n",
    "    '''\n",
    "    This function takes in data formatted by any of the get_{topic}_data functions with LDA = true called.\n",
    "    \n",
    "    n_documents: the number of documents to select from data. \n",
    "    test_size: the proportion of n_documents that should be held out for testing\n",
    "    min_word_count: the minimum number of times a word should appear to be kept in vocabulary\n",
    "    \n",
    "    This function returns id2word and corpus for LDA training and testing\n",
    "    '''\n",
    "    \n",
    "    selected = np.random.choice(len(data), n_documents, replace = False)\n",
    "    subset_data = [data[i] for i in selected]\n",
    "    \n",
    "    docs, one_list, vocab = reducedVocab(subset_docs, min_word_count = min_word_count)\n",
    "    \n",
    "    cut_off = int(np.floor(n_documents * test_size))\n",
    "    train, test = docs[:cut_off], docs[cut_off:]\n",
    "    \n",
    "    id2word = corpora.Dictionary(docs)\n",
    "    train_corpus = [id2word.doc2bow(doc) for doc in train]\n",
    "    test_corpus = [id2word.doc2bow(doc) for doc in test]\n",
    "    \n",
    "    return id2word, train_corpus, test_corpus, test\n",
    "\n",
    "def LDA(id2word, corpus, n_topics):\n",
    "    '''\n",
    "    This function runs gensim's LdaModel.\n",
    "    '''\n",
    "    \n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus,\n",
    "                                               id2word = id2word,\n",
    "                                               num_topics = n_topics,\n",
    "                                               random_state = 23,\n",
    "                                               alpha = 'asymmetric',\n",
    "                                               iterations = 500)\n",
    "    \n",
    "    return lda_model\n",
    "\n",
    "def perplexity(model, test_corpus, test):\n",
    "    '''\n",
    "    This function takes a trained LDA model and calculates the perplexity of the test corpus\n",
    "    '''\n",
    "    \n",
    "    model.get_document_topics(test_corpus, minimum_probability = 1e-8, per_word_topics = True)       \n",
    "    \n",
    "    new_topics = model[test_corpus]\n",
    "    \n",
    "    log_perplex = 0\n",
    "\n",
    "    for i in range(len(test_corpus)):\n",
    "        theta = [e for _, e in new_topics[i][0]]\n",
    "        phi = []\n",
    "        for j in range(len(new_topics[i][2])):\n",
    "            first, second = new_topics[i][2][j]\n",
    "            for k in range(len(theta)):\n",
    "                phi.append([e for _, e in second if _ == k])\n",
    "                if len(phi[j*len(theta) + k]) == 0:\n",
    "                    phi[j*len(theta) + k] = [0]\n",
    "        phi = np.array(phi).reshape(-1, len(theta))\n",
    "        log_perplex -= np.sum(np.log(np.inner(theta, phi)))\n",
    "\n",
    "    N = len([i for sublist in test for i in sublist])\n",
    "\n",
    "    return np.exp(log_perplex / N)\n",
    "\n",
    "def plt_perplexity(perplexity, min_topics, max_topics):\n",
    "    '''\n",
    "    This function plots the perplexity given perplexity array.\n",
    "    \n",
    "    First row of perplexity array is the perplexity values\n",
    "    Second row of perplexity array is the corresponding number of topics used for LDA training\n",
    "    '''\n",
    "    plt.plot(perplexity[1,:], perplexity[0,:])\n",
    "    plt.xlabel('Number of LDA Topics')\n",
    "    plt.ylabel('Perplexity')\n",
    "    plt.title('Perplexity of LDA Model on Test Documents')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nematode(max_docs = None, min_word_count = 1, LDA = False):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the data matrix X and document encodings j from the nematode abstracts\n",
    "    used in the HDP paper.\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://raw.githubusercontent.com/tdhopper/topic-modeling-datasets/master/data/raw/Nematode%20biology%20abstracts/cgcbib.txt'\n",
    "    file = urllib.request.urlopen(url)\n",
    "    data = file.read().decode(\"ISO-8859-1\")\n",
    "    \n",
    "    lists = docsToList(data)\n",
    "   \n",
    "    if max_docs is None:\n",
    "        max_docs = len(lists)\n",
    "    if LDA = False:\n",
    "        return listsToVec(lists[:max_docs], min_word_count=min_word_count)\n",
    "    else:\n",
    "        return reducedVocab(lists[:max_docs], min_word_count = min_word_count)\n",
    "\n",
    "\n",
    "def get_reuters(max_docs = None, min_word_count = 1, LDA = False):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the data matrix X and document encodings j in the Reuters data.\n",
    "    data_dir: a path to the directory containing the pre-downloaded Reuters data.\n",
    "    \"\"\"\n",
    "\n",
    "    #directory = pkgutil.get_data('hdp_py', 'data') #os.fsencode(data_dir)\n",
    "\n",
    "    docs = []\n",
    "    for i in range(22):\n",
    "        suffix = '%03i' % i\n",
    "        #root = directory.decode('ascii')\n",
    "        #filename = os.fsdecode(file)\n",
    "        #print(filename)\n",
    "        #f = open(filename, 'r')\n",
    "\n",
    "        data = pkgutil.get_data('hdp_py', f'data/reut2-{suffix}.sgm')\n",
    "        soup = BeautifulSoup(data, features='lxml')\n",
    "        contents = soup.findAll('text')\n",
    "        #f.close()\n",
    "        docs.append(str(contents).split('</text>'))\n",
    "\n",
    "\n",
    "    docs = [i for doc in docs for i in doc]\n",
    "\n",
    "    # split on </dateline> and keep everything after it\n",
    "\n",
    "    docs = list(compress(docs, ['</dateline>' in i for i in docs]))\n",
    "    docs = [i.split('</dateline>')[1] for i in docs]\n",
    "    docs = [i.lower().translate(str.maketrans('\\n', ' ')) for i in docs]\n",
    "    docs = [i.translate(str.maketrans('\\r', ' ')) for i in docs]\n",
    "    docs = [i.translate(str.maketrans('\\x03', ' ')) for i in docs]\n",
    "    docs = [i.translate(str.maketrans('', '', string.punctuation)) for i in docs]\n",
    "    docs = [i.translate(str.maketrans('', '', string.digits)) for i in docs]\n",
    "    docs = [i.replace('said',' ') for i in docs] # another stop word\n",
    "    docs = [i.replace('reuter', ' ') for i in docs] # the name of the company at the end of most articles\n",
    "    docs = [i.split() for i in docs]\n",
    "\n",
    "    \n",
    "\n",
    "    if max_docs is None:\n",
    "        max_docs = len(docs)\n",
    "\n",
    "    if LDA = False:\n",
    "        return listsToVec(docs[:max_docs], min_word_count=min_word_count)\n",
    "    else:\n",
    "        return reducedVocab(docs[:max_docs], min_word_count = min_word_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
