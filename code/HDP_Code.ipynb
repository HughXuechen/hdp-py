{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDP Gibbs Samplers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Samplers\n",
    "\n",
    "Both of the above schemes are implemented in the `HDP` class and callable via the `gibbs_crf` and `gibbs_direct` methods, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from scipy.special import gammaln as logg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pois_fk_cust(i, x, k, Kmax, ha, hb, new=False):\n",
    "    \"\"\"\n",
    "    Computes the mixture components for a given customer across all k values.\n",
    "    MODEL: base measure H ~ Gamma(ha, hb), F(x|phi) ~ Poisson(phi)\n",
    "    All components are calculated exactly in log-space and then exponentiated.\n",
    "    \n",
    "    returns: (Kmax,) vector; if new=True, returns a scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x.flatten()  # reshape to 1D, since gibbs routine passes in a 2D array\n",
    "    \n",
    "    # Calculate the case where k has no members\n",
    "    fknew_cust = np.exp( -logg(x[i] + 1) + logg(x[i] + ha) - logg(ha) -\n",
    "                         (x[i] + ha)*np.log(1 + hb) + ha*np.log(hb) )\n",
    "    if new == True: return fknew_cust        \n",
    "    \n",
    "    x_kks = [x[k == kk] for kk in range(Kmax)]  # subset of customers eating kk\n",
    "    xi_in = np.zeros(Kmax)                      # offset if x[i] is in this subset\n",
    "    xi_in[k[i]] = 1\n",
    "      \n",
    "    # Compute (a,b) params from gamma kernel tricks done in fk function\n",
    "    av = np.array(list(map(np.sum, x_kks))) - xi_in*x[i] + ha\n",
    "    bv = np.array(list(map(len, x_kks))) - xi_in + hb\n",
    "    fk_cust = np.exp( -logg(x[i] + 1) + logg(x[i] + av) - logg(av) -\n",
    "                      (x[i] + av)*np.log(1 + bv) + av*np.log(bv) )\n",
    "     \n",
    "    return fk_cust\n",
    "\n",
    "\n",
    "def pois_fk_tabl(jj, tt, x, j, t, k, Kmax, ha, hb, new=False):\n",
    "    \"\"\"\n",
    "    Computes the mixture components for a given table across all k values.\n",
    "    MODEL: base measure H ~ Gamma(ha, hb), F(x|phi) ~ Poisson(phi)\n",
    "    All components are calculated exactly in log-space and then exponentiated.\n",
    "    \n",
    "    returns: (Kmax,) vector; if new=True, returns a scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x.flatten()  # reshape to 1D, since gibbs routine passes in a 2D array\n",
    "    x_jt = x[np.logical_and(j == jj, t == tt)]\n",
    "    kk = k[np.logical_and(j == jj, t == tt)]\n",
    "    \n",
    "    fknew_tabl = np.exp( -np.sum(logg(x_jt + 1)) + logg(np.sum(x_jt) + ha) - logg(ha) -\n",
    "                         (np.sum(x_jt) + ha)*np.log(len(x_jt) + hb) + ha*np.log(hb) )\n",
    "    # If table jt doesn't exist, just return the \"new\" mixture component\n",
    "    if len(x_jt) == 0:\n",
    "        #print(f\"WARNING: table {(jj, tt)} does not exist currently\")\n",
    "        new = True\n",
    "    if new == True: return np.full(Kmax, fknew_tabl)\n",
    "    \n",
    "    x_kks = [x[k == kk] for kk in range(Kmax)]  # subset of customers at tables serving kk\n",
    "    xjt_in = np.zeros(Kmax)                     # offset if table x_jt is in this subset\n",
    "    xjt_in[kk[0]] = 1\n",
    "      \n",
    "    # Compute (a,b) params from gamma kernel tricks done in fk function\n",
    "    av = np.array(list(map(np.sum, x_kks))) - xjt_in*np.sum(x_jt) + ha\n",
    "    bv = np.array(list(map(len, x_kks))) - xjt_in*len(x_jt) + hb\n",
    "    fk_tabl = np.exp( -np.sum(logg(x_jt + 1)) + logg(np.sum(x_jt) + av) - logg(av) -\n",
    "                       (np.sum(x_jt) + av)*np.log(len(x_jt) + bv) + av*np.log(bv) )\n",
    "     \n",
    "    return fk_tabl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnom_fk_cust(i, x, k, Kmax, L, ha, new=False):\n",
    "    \"\"\"\n",
    "    Computes the mixture components for a given customer across all k values.\n",
    "    MODEL: base measure H ~ Dirichlet(L, ha_1,...,ha_L),\n",
    "                        F(x|phi) ~ Multinomial(n_ji, phi_1,...,phi_L)\n",
    "    All components are calculated exactly in log-space and then exponentiated.\n",
    "    X can be a dense or a sparse csr-style matrix.\n",
    "    \n",
    "    returns: (Kmax,) vector; if new=True, returns a scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    xi, ni = x[i, :], np.sum(x[i, :])\n",
    "    log_con = logg(ni + 1) - np.sum(logg(xi + np.ones(L))) # term constant for all k\n",
    "    # Calculate the case where k has no members\n",
    "    \n",
    "    if new == True:\n",
    "        fknew_cust = np.exp( log_con + np.sum(logg(xi + ha)) - logg(np.sum(xi + ha)) + \n",
    "                             logg(np.sum(ha)) - np.sum(logg(ha)) )\n",
    "        return fknew_cust        \n",
    "    \n",
    "    # Get subset of customers eating kk; each entry is a (#, L) matrix\n",
    "    x_kks = [x[k == kk, :] for kk in range(Kmax)]  \n",
    "    \n",
    "    # Compute params from Dirichlet kernel tricks done in fk function\n",
    "    a_bot = np.vstack([np.sum(x_kk, axis=0) for x_kk in x_kks]) + ha[None, :]    # (Kmax, L)\n",
    "    a_bot[k[i], :] -= xi                         # offset if xi is in this subset\n",
    "    a_top = np.apply_along_axis(lambda row: row + xi, 1, a_bot)\n",
    "    fk_cust = np.exp( log_con + np.sum(logg(a_top), axis=1) - logg(np.sum(a_top, axis=1)) +\n",
    "                      logg(np.sum(a_bot, axis=1)) - np.sum(logg(a_bot), axis=1) )\n",
    "     \n",
    "    # Convert back to a dense array in case X was sparse\n",
    "    return np.asarray(fk_cust).ravel()\n",
    "\n",
    "\n",
    "def mnom_fk_tabl(jj, tt, x, j, t, k, Kmax, L, ha, new=False):\n",
    "    \"\"\"\n",
    "    Computes the mixture components for a given customer across all k values.\n",
    "    MODEL: base measure H ~ Dirichlet(L, ha_1,...,ha_L),\n",
    "                        F(x|phi) ~ Multinomial(n_ji, phi_1,...,phi_L)\n",
    "    All components are calculated exactly in log-space and then exponentiated.\n",
    "    \n",
    "    returns: (Kmax,) vector; if new=True, returns a scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    x_jt = x[np.logical_and(j == jj, t == tt), :]                                # (|T|, L)\n",
    "    kk = k[np.logical_and(j == jj, t == tt)]\n",
    "    n_jt = np.sum(x_jt, axis=1)                                                  # (|T|,)\n",
    "    sum_jt = np.sum(x_jt, axis=0)                                                # (L,)\n",
    "    log_con = np.sum(logg(n_jt + 1)) - np.sum(logg(x_jt + 1))    # term constant for all k\n",
    "    \n",
    "    fknew_tabl = np.exp( log_con + np.sum(logg(sum_jt + ha)) - logg(np.sum(sum_jt + ha)) + \n",
    "                         logg(np.sum(ha)) - np.sum(logg(ha)) )\n",
    "    # If table jt doesn't exist, just return the \"new\" mixture component\n",
    "    if x_jt.shape[0] == 0:\n",
    "        #print(f\"WARNING: table {(jj, tt)} does not exist currently\")\n",
    "        new = True\n",
    "    if new == True: return fknew_tabl       \n",
    "    \n",
    "    # Get subset of customers eating kk; each entry is a (#, L) matrix\n",
    "    x_kks = [x[k == kk, :] for kk in range(Kmax)]\n",
    "      \n",
    "    # Compute params from Dirichlet kernel tricks done in fk function\n",
    "    a_bot = np.vstack([np.sum(x_kk, axis=0) for x_kk in x_kks]) + ha[None, :]    # (Kmax, L)\n",
    "    a_bot[kk[0], :] -= sum_jt                       # offset if table x_jt is in this subset\n",
    "    a_top = a_bot + sum_jt[None, :]\n",
    "    fk_tabl = np.exp( log_con + np.sum(logg(a_top), axis=1) - logg(np.sum(a_top, axis=1)) +\n",
    "                      logg(np.sum(a_bot, axis=1)) - np.sum(logg(a_bot), axis=1) )\n",
    "\n",
    "    return fk_tabl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_fk_cust(i, x, k, Kmax, L, ha, new=False):\n",
    "    \"\"\"\n",
    "    Computes the mixture components for a given customer across all k values.\n",
    "    MODEL: base measure H ~ Dirichlet(L, ha_1,...,ha_L),\n",
    "                        F(x|phi) ~ Categorical(L, phi_1,...,phi_L)\n",
    "    All components are calculated exactly in log-space and then exponentiated.\n",
    "    X can be a dense or a sparse csr-style matrix.\n",
    "    \n",
    "    returns: (Kmax,) vector; if new=True, returns a scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    xi = x[i, :]\n",
    "    ll = sparse.find(xi)[0][0]        # get index of the 1 value\n",
    "    # Calculate the case where k has no members\n",
    "    if new == True:\n",
    "        return ha[ll] / np.sum(ha)    \n",
    "    \n",
    "    # Store the size of sets V and V_l for each k\n",
    "    V_kks = np.array([np.sum(k == kk) for kk in range(Kmax)])\n",
    "    Vl_kks = np.array([np.sum(x[k == kk, ll]) for kk in range(Kmax)])\n",
    "    \n",
    "    fk_cust = (Vl_kks + ha[ll]) / (V_kks + np.sum(ha))\n",
    "    return fk_cust\n",
    "\n",
    "\n",
    "def cat_fk_tabl(jj, tt, x, j, t, k, Kmax, L, ha, new=False):\n",
    "    \"\"\"\n",
    "    Computes the mixture components for a given customer across all k values.\n",
    "    MODEL: base measure H ~ Dirichlet(L, ha_1,...,ha_L),\n",
    "                        F(x|phi) ~ Categorical(L, phi_1,...,phi_L)\n",
    "    All components are calculated exactly in log-space and then exponentiated.\n",
    "    \n",
    "    returns: (Kmax,) vector; if new=True, returns a scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    x_jt = x[np.logical_and(j == jj, t == tt), :]                                # (|T|, L)\n",
    "    kk = k[np.logical_and(j == jj, t == tt)]\n",
    "    n_jt = np.sum(x_jt, axis=1)                                                  # (|T|,)\n",
    "    sum_jt = np.sum(x_jt, axis=0)                                                # (L,)\n",
    "    log_con = np.sum(logg(n_jt + 1)) - np.sum(logg(x_jt + 1))    # term constant for all k\n",
    "    \n",
    "    fknew_tabl = np.exp( log_con + np.sum(logg(sum_jt + ha)) - logg(np.sum(sum_jt + ha)) + \n",
    "                         logg(np.sum(ha)) - np.sum(logg(ha)) )\n",
    "    # If table jt doesn't exist, just return the \"new\" mixture component\n",
    "    if x_jt.shape[0] == 0:\n",
    "        print(f\"WARNING: table {(jj, tt)} does not exist currently\")\n",
    "        new = True\n",
    "    if new == True: return fknew_tabl       \n",
    "    \n",
    "    # Get subset of customers eating kk; each entry is a (#, L) matrix\n",
    "    x_kks = [x[k == kk, :] for kk in range(Kmax)]\n",
    "      \n",
    "    # Compute params from Dirichlet kernel tricks done in fk function\n",
    "    a_bot = np.vstack([np.sum(x_kk, axis=0) for x_kk in x_kks]) + ha[None, :]    # (Kmax, L)\n",
    "    a_bot[kk[0], :] -= sum_jt                       # offset if table x_jt is in this subset\n",
    "    a_top = a_bot + sum_jt[None, :]\n",
    "    fk_tabl = np.exp( log_con + np.sum(logg(a_top), axis=1) - logg(np.sum(a_top, axis=1)) +\n",
    "                      logg(np.sum(a_bot, axis=1)) - np.sum(logg(a_bot), axis=1) )\n",
    "\n",
    "    return fk_tabl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StirlingEngine:\n",
    "    \"\"\"\n",
    "    Numerically efficient engine for computing and storing computed Stirling numbers.\n",
    "    \n",
    "    CONSTRUCTOR PARAMETERS\n",
    "    - Nmax: largest integer n for which s(n,m) will need to be computed\n",
    "    \n",
    "    PRIVATE ATTRIBUTES\n",
    "    - s_memo_, slog_memo_: running tables of previously computed values\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, Nmax):\n",
    "        self.s_memo_ = np.full((Nmax, Nmax), np.nan)\n",
    "        self.slog_memo_ = np.full((Nmax, Nmax), np.nan)\n",
    "        \n",
    "        \n",
    "    def stirling(self, n, m):\n",
    "        \"\"\"\n",
    "        Computes an unsigned Stirling number of the first kind.\n",
    "        Uses dynamic programming to store previously computed s(n,m) values,\n",
    "        as this is a repeatedly-called recursive algorithm.\n",
    "        \"\"\"\n",
    "        \n",
    "        assert n < self.s_memo_.shape[0] and m < self.s_memo_.shape[0]  \n",
    "        # If this has already been computed, return stored value\n",
    "        if not np.isnan(self.s_memo_[n, m]):\n",
    "            return self.s_memo_[n, m]\n",
    "        else:\n",
    "            return_val = np.nan\n",
    "\n",
    "            # Base cases\n",
    "            if (n == 0 and m == 0) or (n == 1 and m == 1):\n",
    "                return_val = 1\n",
    "            elif (n > 0 and m == 0) or m > n:\n",
    "                return_val = 0\n",
    "            # Recursion relation\n",
    "            else:\n",
    "                return_val = self.stirling(n-1, m-1) + (n-1)*self.stirling(n-1, m)\n",
    "\n",
    "            self.s_memo_[n, m] = return_val\n",
    "            return return_val\n",
    "    \n",
    "    \n",
    "    def stirlog(self, n, m):\n",
    "        \"\"\"\n",
    "        Computes the natural logarithm of an unsigned Stirling number,\n",
    "        using the same dynamic programming approach as above.\n",
    "        If s(n,m) = 0, this gets returned as -inf (np.exp(-inf) == 0.0)\n",
    "        \n",
    "        This is the preferred function, as stirling() can encounter overflow errors.\n",
    "        \"\"\"\n",
    "        \n",
    "        assert n < self.slog_memo_.shape[0] and m < self.slog_memo_.shape[0]  \n",
    "        # If this has already been computed, return stored value\n",
    "        if not np.isnan(self.slog_memo_[n, m]):\n",
    "            return self.slog_memo_[n, m]\n",
    "        else:\n",
    "            return_val = np.nan\n",
    "\n",
    "            # Base cases\n",
    "            if (n == 0 and m == 0) or (n == 1 and m == 1):\n",
    "                return_val = 0\n",
    "            elif (n > 0 and m == 0) or m > n:\n",
    "                return_val = -np.inf\n",
    "            # Recursion relation\n",
    "            else:\n",
    "                log_s1, log_s2 = self.stirlog(n-1, m-1), self.stirlog(n-1, m)\n",
    "                # If s1 == 0 (log_s1 == -inf), just return (n-1)*log_s2\n",
    "                # By definition, must have s2 > s1, so only need to check s1\n",
    "                if np.isfinite(log_s1):\n",
    "                    val = (n-1) * np.exp(log_s2 - log_s1)\n",
    "                    # If there is overflow/underflow in `val`, approximate log(1+x) = log(x)\n",
    "                    if np.isfinite(val):\n",
    "                        return_val = log_s1 + np.log1p(val)\n",
    "                    else:\n",
    "                        return_val = log_s2 + np.log(n-1)\n",
    "                else:\n",
    "                    return_val = log_s2 + np.log(n-1)\n",
    "\n",
    "            self.slog_memo_[n, m] = return_val\n",
    "            return return_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDP:\n",
    "    \"\"\"\n",
    "    Model implementing the Chinese Restaurant Franchise Process formulation of the HDP.\n",
    "    \n",
    "    CONSTRUCTOR PARAMETERS\n",
    "    - gamma, alpha0: scaling parameters > 0 for base measures H and G0\n",
    "    - f: string representing distribution of data; h is chosen to be conjugate\n",
    "    - hypers: tuple of hyperparameter values specific to f/h scheme chosen\n",
    "    \n",
    "    PRIVATE ATTRIBUTES (volatile)\n",
    "    - tk_map_: (J x Tmax) matrix of k values for each (j,t) pair\n",
    "    - beta_: (Kmax + 1,) vector of beta values for each k\n",
    "    - n_: (J x Tmax) matrix specifying counts of customers (gibbs_cfr)\n",
    "    - q_: (J x Kmax) matrix specifying counts of customers (gibbs_direct)\n",
    "    - m_: (J x Kmax) matrix specifying counts of tables\n",
    "    - fk_cust_, fk_tabl_: functions to compute mixing components for Gibbs sampling\n",
    "    - stir_: an object of class StirlingEngine which computes Stirling numbers\n",
    "    \n",
    "    PUBLIC ATTRIBUTES\n",
    "    cfr_samples: (S x N x 2) matrix of (t, k) values for each data point i;\n",
    "                 exists only after gibbs_cfr() has been called\n",
    "    direct_samples: (S x N) matrix of k values for each data point i;\n",
    "                    exists only after gibbs_direct() has been called\n",
    "    beta_samples: (S x Kmax+1) matrix of beta values after each iteration;\n",
    "                  exists only after gibbs_direct() has been called\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=1, alpha0=1, f='multinomial', hypers=None):\n",
    "        self.g_ = gamma\n",
    "        self.a0_ = alpha0\n",
    "        self.set_priors(f, hypers)\n",
    "        \n",
    "    def set_priors(self, f, hypers):\n",
    "        \"\"\"\n",
    "        Initializes the type of base measure h_ and data-generation function f_.\n",
    "        Also sets hypers_, the relevelant hyperparameters and\n",
    "                  fk_routine_, the function to compute mixing components.\n",
    "        \"\"\"\n",
    "        if f == 'poisson':\n",
    "            # Specify parameters of H ~ Gamma(a,b)\n",
    "            if hypers is None:\n",
    "                self.hypers_ = (1,1)\n",
    "            else: self.hypers_ = hypers\n",
    "            self.fk_cust_ = pois_fk_cust\n",
    "            self.fk_tabl_ = pois_fk_tabl\n",
    "        \n",
    "        elif f == 'multinomial':\n",
    "            if hypers is None:\n",
    "                L = 2\n",
    "                self.hypers_ = (L, np.ones(L))\n",
    "            else: self.hypers_ = hypers\n",
    "            self.fk_cust_ = mnom_fk_cust\n",
    "            self.fk_tabl_ = mnom_fk_tabl\n",
    "            \n",
    "        elif f == 'categorical':\n",
    "            # Identical to multinomial, but with some efficiency upgrades\n",
    "            if hypers is None:\n",
    "                L = 2\n",
    "                self.hypers_ = (L, np.ones(L))\n",
    "            else: self.hypers_ = hypers\n",
    "            self.fk_cust_ = cat_fk_cust\n",
    "            self.fk_tabl_ = mnom_fk_tabl\n",
    "    \n",
    "    \n",
    "    def tally_up(self, it, which=None):\n",
    "        \"\"\"\n",
    "        Helper function for computing maps and counts in gibbs().\n",
    "        Given a current iteration in the cfr_samples attribute, does a full\n",
    "        recount of customer/table allocations, updating n_ and m_.\n",
    "        Set which = 'n' or 'm' to only tally up that portion\n",
    "        \"\"\"    \n",
    "        \n",
    "        if which == 'n':\n",
    "            jt_pairs = self.cfr_samples[it,:,0:2]\n",
    "            # Count customers at each table (jt)\n",
    "            cust_counts = pd.Series(map(tuple, jt_pairs)).value_counts()\n",
    "            j_idx, t_idx = tuple(map(np.array, zip(*cust_counts.index)))\n",
    "            self.n_ *= 0\n",
    "            self.n_[j_idx, t_idx] = cust_counts\n",
    "            \n",
    "        elif which == 'm':\n",
    "            jt_pairs = self.cfr_samples[it,:,0:2]\n",
    "            # First filter by unique tables (jt), then count tables with each k value\n",
    "            jt_unique, k_idx = np.unique(jt_pairs, axis=0, return_index=True)\n",
    "            jk_pairs = np.c_[self.cfr_samples[it, k_idx, 0],\n",
    "                             self.cfr_samples[it, k_idx, 2]]\n",
    "            #print(jk_pairs)\n",
    "            tabl_counts = pd.Series(map(tuple, jk_pairs)).value_counts()\n",
    "            #print(tabl_counts)\n",
    "            j_idx, k_idx = tuple(map(np.array, zip(*tabl_counts.index)))\n",
    "            self.m_ *= 0\n",
    "            self.m_[j_idx, k_idx] = tabl_counts\n",
    "            \n",
    "        elif which == 'q':\n",
    "            jk_pairs = self.direct_samples[it,:,:]\n",
    "            # Counts customers at each j eating k\n",
    "            cust_counts = pd.Series(map(tuple, jk_pairs)).value_counts()\n",
    "            j_idx, k_idx = tuple(map(np.array, zip(*cust_counts.index)))\n",
    "            self.q_ *= 0\n",
    "            self.q_[j_idx, k_idx] = cust_counts\n",
    "            \n",
    "    \n",
    "    def get_dist(self, old, new, used, size):\n",
    "        \"\"\"\n",
    "        Helper function which standardizes the operation of computing a\n",
    "        full conditional distribution, for both t and k values.\n",
    "        Also normalizes and ensures there are no NANs.\n",
    "        - old: a (size,) vector of probability values for used values\n",
    "        - new: a scalar representing the combined probability of all unused values\n",
    "        - used: a (size,) mask encoding which values in the sample space are being used\n",
    "        - size: the size of the sample space\n",
    "        \"\"\"\n",
    "        \n",
    "        num_unused = size - np.sum(used)\n",
    "        dist = None\n",
    "        if num_unused == 0:\n",
    "            # In our truncated sample space, there is no room for \"new\" values\n",
    "            dist = old\n",
    "        else:\n",
    "            dist = old * used + (new / num_unused) * np.logical_not(used)\n",
    "        \n",
    "        # Remove nans and add epsilon so that distribution is all positive\n",
    "        dist[np.isnan(dist)] = 0\n",
    "        dist += 1e-10\n",
    "        return dist / np.sum(dist)\n",
    "    \n",
    "    \n",
    "    def draw_t(self, it, x, j, Tmax, Kmax, verbose):\n",
    "        \"\"\"\n",
    "        Helper function which does the draws from the t_ij full conditional.\n",
    "        Updates the counts and the samples matrices at iteration `it`.\n",
    "        Called by gibbs_cfr()\n",
    "        \"\"\"\n",
    "        \n",
    "        t_next, k_next = self.cfr_samples[it,:,1], self.cfr_samples[it,:,2]\n",
    "        # Cycle through the t value of each customer, conditioning on everything\n",
    "        # Randomize the order in which updates occur\n",
    "        for i in np.random.permutation(len(j)):\n",
    "            jj, tt0, kk0 = j[i], t_next[i], k_next[i]\n",
    "\n",
    "            # Get vector of customer f_k values (dependent on model specification)\n",
    "            old_mixes = self.fk_cust_(i, x, k_next, Kmax, *self.hypers_) \n",
    "            new_mixes = self.fk_cust_(i, x, k_next, Kmax, *self.hypers_, new=True) \n",
    "            # Calculate pointwise likelihoods p(x_ji | ...)\n",
    "            M = np.sum(self.m_)\n",
    "            Mk = np.sum(self.m_, axis=0)   # number of tables serving k\n",
    "            lik = old_mixes @ (Mk / (M + self.g_)) + new_mixes * (self.g_ / (M + self.g_))\n",
    "\n",
    "            cust_offset = np.zeros(Tmax)\n",
    "            cust_offset[tt0] = 1\n",
    "            old_t = (self.n_[jj, :] - cust_offset) * old_mixes[self.tk_map_[jj, :]]      \n",
    "            new_t = self.a0_ * lik\n",
    "            # If a table is in use, prob comes from old_t; otherwise, from new_t\n",
    "            # Distribute the weight of new_t across all possible new allocations\n",
    "            t_used = self.n_[jj, :] > 0\n",
    "            t_dist = self.get_dist(old_t, new_t, t_used, Tmax)\n",
    "\n",
    "            tt1 = np.random.choice(Tmax, p=t_dist)\n",
    "            t_next[i] = tt1\n",
    "            self.tally_up(it, which='n')\n",
    "\n",
    "            # If this table was previously unoccupied, we need to select a k\n",
    "            if self.n_[jj, tt1] == 1 and tt0 != tt1:\n",
    "                old_k = np.sum(self.m_, axis=0) * old_mixes\n",
    "                new_k = self.g_ * new_mixes\n",
    "                k_used = np.sum(self.m_, axis=0) > 0\n",
    "                k_dist = self.get_dist(old_k, new_k, k_used, Kmax)\n",
    "\n",
    "                kk1 = np.random.choice(Kmax, p=k_dist)\n",
    "                self.tk_map_[jj, tt1] = kk1\n",
    "                k_next[i] = self.tk_map_[jj, tt1]\n",
    "            self.tally_up(it, which='m')\n",
    "\n",
    "            #if verbose: print(f\"~ customer (j,i) = {(jj,i)}\" +\n",
    "                              #f\" moves table: {tt0} -> {t_next[i]}, k: {kk0} -> {k_next[i]}\")\n",
    "    \n",
    "    \n",
    "    def draw_k(self, it, x, j, Kmax, verbose):\n",
    "        \"\"\"\n",
    "        Helper function which does the draws from the t_ij full conditional.\n",
    "        Updates the counts and the samples matrices at iteration `it`.\n",
    "        Called by gibbs_cfr()\n",
    "        \"\"\"\n",
    "        \n",
    "        t_next, k_next = self.cfr_samples[it,:,1], self.cfr_samples[it,:,2]\n",
    "        # Cycle through the k values of each table\n",
    "        j_idx, t_idx = np.where(self.n_ > 0)   # find the occupied tables\n",
    "        for i in np.random.permutation(len(j_idx)):\n",
    "            jj, tt = j_idx[i], t_idx[i]\n",
    "            kk0 = self.tk_map_[jj, tt]\n",
    "\n",
    "            # Get vector of table f_k values (dependent on model specification)\n",
    "            old_mixes = self.fk_tabl_(jj, tt, x, j, t_next, k_next, Kmax, *self.hypers_) \n",
    "            new_mixes = self.fk_tabl_(jj, tt, x, j, t_next, k_next, Kmax, *self.hypers_, new=True) \n",
    "\n",
    "            tabl_offset = np.zeros(Kmax)\n",
    "            tabl_offset[kk0] = 1\n",
    "            old_k = (np.sum(self.m_, axis=0) - tabl_offset) * old_mixes\n",
    "            new_k = self.g_ * new_mixes\n",
    "            k_used = np.sum(self.m_, axis=0) > 0\n",
    "            k_dist = self.get_dist(old_k, new_k, k_used, Kmax)\n",
    "\n",
    "            kk1 = np.random.choice(Kmax, p=k_dist)\n",
    "            self.tk_map_[jj, tt] = kk1\n",
    "            k_next[np.logical_and(j == jj, t_next == tt)] = kk1\n",
    "            self.tally_up(it, which='m')\n",
    "\n",
    "            #if verbose: print(f\"~~ table (j,t) = {(jj,tt)} changes dish: {kk0} -> {kk1}\")\n",
    "    \n",
    "    \n",
    "    def draw_z(self, it, x, j, Kmax, verbose):\n",
    "        \"\"\"\n",
    "        Helper function which does the draws from the z_ij full conditional.\n",
    "        Updates the counts and the samples matrices at iteration `it`.\n",
    "        Called by gibbs_direct()\n",
    "        \"\"\"\n",
    "        \n",
    "        k_next = self.direct_samples[it,:,1]\n",
    "        # Cycle through the k values of each customer\n",
    "        for i in np.random.permutation(len(j)):\n",
    "            jj, kk0 = j[i], k_next[i]\n",
    "            \n",
    "            # Get vector of customer f_k values (dependent on model specification)\n",
    "            old_mixes = self.fk_cust_(i, x, k_next, Kmax, *self.hypers_) \n",
    "            new_mixes = self.fk_cust_(i, x, k_next, Kmax, *self.hypers_, new=True) \n",
    "            \n",
    "            cust_offset = np.zeros(Kmax)\n",
    "            cust_offset[kk0] = 1\n",
    "            old_k = (self.q_[jj, :] - cust_offset +\n",
    "                     self.a0_ * self.beta_samples[it, :-1]) * old_mixes      \n",
    "            new_k = self.a0_ * self.beta_samples[it, -1] * new_mixes\n",
    "            k_used = np.sum(self.m_, axis=0) > 0\n",
    "            k_dist = self.get_dist(old_k, new_k, k_used, Kmax)\n",
    "\n",
    "            kk1 = np.random.choice(Kmax, p=k_dist)\n",
    "            k_next[i] = kk1\n",
    "            self.tally_up(it, which='q')\n",
    "            \n",
    "            # If this k value was previously unused, must also set the beta_k component\n",
    "            if np.sum(self.q_[:, kk1] == 1):\n",
    "                b = np.random.beta(1, self.g_)\n",
    "                beta_u = self.beta_samples[it, -1]\n",
    "                self.beta_samples[it, kk1] = b * beta_u\n",
    "                self.beta_samples[it, -1] = (1-b) * beta_u\n",
    "            \n",
    "            #if verbose:\n",
    "                #print(f\"~ customer (j,i) = {(jj,i)}\" +\n",
    "                      #f\"changes dish: {kk0} -> {kk1}\")\n",
    "                #print(f\"  k_dist: {k_dist.round(3)} (sum {np.sum(k_dist)})\")\n",
    "                \n",
    "    \n",
    "    def draw_m(self, it, x, j, Kmax, verbose):\n",
    "        \"\"\"\n",
    "        Helper function which does the draws from the z_ij full conditional.\n",
    "        Updates the counts and the samples matrices at iteration `it`.\n",
    "        Called by gibbs_direct()\n",
    "        \"\"\"\n",
    "        \n",
    "        k_next = self.direct_samples[it,:,1]\n",
    "        self.m_ *= 0                           # reset the m counts\n",
    "        # Cycle through the k values of each restaurant\n",
    "        j_idx, k_idx = np.where(self.q_ > 0)   # find the consumed dishes\n",
    "        for i in np.random.permutation(len(j_idx)):\n",
    "            jj, kk = j_idx[i], k_idx[i]\n",
    "            max_m = self.q_[jj, kk]\n",
    "            \n",
    "            abk = self.a0_ * self.beta_samples[it, kk]\n",
    "            m_range = np.arange(max_m) + 1\n",
    "            log_s = np.array([self.stir_.stirlog(max_m, m) for m in m_range])\n",
    "            m_dist = np.exp( logg(abk) - logg(abk + max_m) +\n",
    "                             log_s + m_range * np.log(abk) )\n",
    "            \"\"\"MOSTLY FIXED.  m_dist should be a proper distribution\"\"\"\n",
    "            m_dist[np.isnan(m_dist)] = 0\n",
    "            m_dist += 1e-6\n",
    "            \n",
    "            mm1 = np.random.choice(m_range, p=m_dist/np.sum(m_dist))\n",
    "            self.m_[jj, kk] = mm1\n",
    "\n",
    "            #if verbose:\n",
    "                #print(f\"~~ restaraunt {jj}: {mm1} tables / {max_m} customers eating {kk}\")\n",
    "                #print(f\"m_dist: {m_dist.round(3)}\")\n",
    "                \n",
    "    \n",
    "    def gibbs_cfr(self, x, j, iters, Tmax=None, Kmax=None, resume=False, verbose=False):\n",
    "        \"\"\"\n",
    "        Runs the Gibbs sampler to generate posterior estimates of t and k.\n",
    "        x: data matrix, stored row-wise if multidimensional\n",
    "        j: vector of group labels; must have same #rows as x\n",
    "        iters: number of iterations to run\n",
    "        Tmax: maximum number of clusters for each group\n",
    "        Kmax: maximum number of atoms to draw from base measure H\n",
    "        resume: if True, will continue from end of previous direct_samples, if dimensions match up\n",
    "        \n",
    "        returns: this HDP object with cfr_samples attribute\n",
    "        \"\"\"\n",
    "        \n",
    "        group_counts = pd.Series(j).value_counts()\n",
    "        J, N = np.max(j) + 1, len(j)\n",
    "        # Set default Tmax and Kmax, if not provided\n",
    "        if Tmax is None: Tmax = min(100, np.max(group_counts))\n",
    "        if Kmax is None: Kmax = min(100, N)\n",
    "            \n",
    "        self.n_ = np.zeros((J, Tmax), dtype='int')\n",
    "        self.m_ = np.zeros((J, Kmax), dtype='int')\n",
    "        self.cfr_samples = np.zeros((iters+1, N, 3), dtype='int')\n",
    "        self.cfr_samples[:,:,0] = j\n",
    "        np.seterr('ignore')\n",
    "        \n",
    "        # Set random initial values for t and k assignments\n",
    "        t0, k0 = self.cfr_samples[0,:,1], self.cfr_samples[0,:,2]\n",
    "        t0[:] = np.random.randint(0, Tmax, size=N)\n",
    "        self.tk_map_ = np.random.randint(0, Kmax//2, (J, Tmax))\n",
    "        self.tally_up(it=0, which='n')\n",
    "        for jj in range(J):\n",
    "            for tt in np.where(self.n_[jj, :] > 0)[0]:\n",
    "                #print(f\"mapping: {(jj, tt)} -> {self.tk_map_[jj, tt]}\")\n",
    "                k0[np.logical_and(j == jj, t0 == tt)] = self.tk_map_[jj, tt]\n",
    "        self.tally_up(it=0, which='m')\n",
    "        \n",
    "        for s in range(iters):\n",
    "            t_prev, k_prev = self.cfr_samples[s,:,1], self.cfr_samples[s,:,2]\n",
    "            t_next, k_next = self.cfr_samples[s+1,:,1], self.cfr_samples[s+1,:,2]\n",
    "            # Copy over the previous iteration as a starting point\n",
    "            t_next[:], k_next[:] = t_prev, k_prev\n",
    "            \n",
    "            self.draw_t(s+1, x, j, Tmax, Kmax, verbose)\n",
    "            self.draw_k(s+1, x, j, Kmax, verbose)\n",
    "        \n",
    "        self.cfr_samples = self.cfr_samples[1:,:,1:]\n",
    "        return self  \n",
    "    \n",
    "    \n",
    "    def gibbs_direct(self, x, j, iters, Kmax=None, resume=False, verbose=False):\n",
    "        \"\"\"\n",
    "        Runs the Gibbs sampler to generate posterior estimates of k.\n",
    "        x: data matrix, stored row-wise if multidimensional\n",
    "        j: vector of group labels; must have same #rows as x\n",
    "        iters: number of iterations to run\n",
    "        Kmax: maximum number of atoms to draw from base measure H\n",
    "        resume: if True, will continue from end of previous direct_samples, if dimensions match up\n",
    "        \n",
    "        returns: this HDP object with direct_samples attribute\n",
    "        \"\"\"\n",
    "        \n",
    "        prev_direct, prev_beta = None, None\n",
    "        start = 0\n",
    "        if resume == True:\n",
    "            # Make sure the x passed in is the same size as it previously was\n",
    "            assert (x.shape[0] == self.direct_samples.shape[1] and\n",
    "                    Kmax == self.beta_samples.shape[1] - 1), \"Cannot resume with different data.\"\n",
    "            iters += self.direct_samples.shape[0]\n",
    "            prev_direct, prev_beta = self.direct_samples, self.beta_samples\n",
    "            start = self.direct_samples.shape[0]\n",
    "        \n",
    "        group_counts = pd.Series(j).value_counts()\n",
    "        J, N = np.max(j) + 1, len(j)\n",
    "        if Kmax is None: Kmax = min(100, N)\n",
    "            \n",
    "        self.direct_samples = np.zeros((iters+1, N, 2), dtype='int')\n",
    "        self.direct_samples[:,:,0] = j\n",
    "        self.beta_samples = np.zeros((iters+1, Kmax+1))\n",
    "        self.stir_ = StirlingEngine(np.max(group_counts) + 1)\n",
    "        np.seterr('ignore')\n",
    "        \n",
    "        if resume == True:\n",
    "            # Fill in the start of the samples with the previously computed samples\n",
    "            self.direct_samples[1:start+1,:,1] = prev_direct\n",
    "            self.beta_samples[1:start+1,:] = prev_beta\n",
    "            # q_ and m_ attributes should already still exist within the object\n",
    "        else:\n",
    "            self.q_ = np.zeros((J, Kmax), dtype='int')   # performs the same function as n_\n",
    "            self.m_ = np.zeros((J, Kmax), dtype='int')\n",
    "            \n",
    "            # Set random initial values for k assignments\n",
    "            k0 = self.direct_samples[0,:,1]\n",
    "            k0[:] = np.random.randint(0, Kmax, size=N)\n",
    "            self.tally_up(it=0, which='q')\n",
    "            # Implicitly set random t assignments by drawing possible m counts (m_jk <= q_jk)\n",
    "            for jj in range(J):\n",
    "                for kk in range(Kmax):\n",
    "                    max_m = self.q_[jj, kk]\n",
    "                    if max_m == 1:\n",
    "                        self.m_[jj, kk] = 1\n",
    "                    elif max_m > 1:\n",
    "                        self.m_[jj, kk] = np.random.randint(1, max_m)\n",
    "            # Compute the corresponding beta values from m assignments\n",
    "            Mk = np.sum(self.m_, axis=0)\n",
    "            self.beta_samples[0,:] = np.random.dirichlet(np.append(Mk, self.g_) + 1e-10)\n",
    "        \n",
    "        for s in range(start, iters):\n",
    "            # Copy over the previous iteration as a starting point\n",
    "            self.direct_samples[s+1,:,1] = self.direct_samples[s,:,1] \n",
    "            self.beta_samples[s+1,:] = self.beta_samples[s,:]\n",
    "            \n",
    "            self.draw_z(s+1, x, j, Kmax, verbose)\n",
    "            self.draw_m(s+1, x, j, Kmax, verbose)\n",
    "            \n",
    "            Mk = np.sum(self.m_, axis=0)\n",
    "            # Dirichlet weights must be > 0, so in case some k is unused, add epsilon\n",
    "            self.beta_samples[s+1,:] = np.random.dirichlet(np.append(Mk, self.g_) + 1e-10)\n",
    "            if verbose: print(self.beta_samples[s+1,:].round(3))\n",
    "        \n",
    "        self.direct_samples = self.direct_samples[1:,:,1]\n",
    "        self.beta_samples = self.beta_samples[1:,:]\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.65 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 200)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulated data (Poisson example)\n",
    "N = 200\n",
    "np.random.seed(0)\n",
    "j = np.random.randint(0, 9, N)\n",
    "x = np.random.poisson(j, N)\n",
    "data = np.c_[x, j]\n",
    "\n",
    "%time c = HDP(f='poisson', hypers=(1,10)).gibbs_direct(x[:,None], j, iters=10, Kmax=10)\n",
    "c.direct_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.58 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20, 200)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time c.gibbs_direct(x[:,None], j, iters=10, resume=True, Kmax=10)\n",
    "c.direct_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.098 0.127 0.254 0.035 0.076 0.039 0.159 0.026 0.066 0.119 0.001]\n",
      " [0.    0.    0.239 0.085 0.119 0.052 0.285 0.026 0.047 0.087 0.06 ]\n",
      " [0.    0.031 0.01  0.043 0.198 0.046 0.113 0.152 0.039 0.355 0.013]\n",
      " [0.    0.044 0.027 0.145 0.073 0.043 0.221 0.048 0.034 0.215 0.15 ]\n",
      " [0.    0.179 0.019 0.181 0.039 0.    0.254 0.005 0.04  0.279 0.006]\n",
      " [0.    0.151 0.062 0.014 0.    0.    0.429 0.    0.053 0.2   0.092]\n",
      " [0.    0.04  0.122 0.01  0.    0.    0.473 0.    0.    0.354 0.002]\n",
      " [0.    0.253 0.015 0.041 0.    0.    0.415 0.    0.    0.135 0.141]\n",
      " [0.    0.012 0.071 0.069 0.    0.    0.333 0.    0.    0.331 0.185]\n",
      " [0.    0.    0.258 0.036 0.    0.    0.392 0.    0.    0.226 0.088]\n",
      " [0.    0.    0.046 0.051 0.    0.    0.131 0.    0.    0.665 0.106]\n",
      " [0.    0.044 0.117 0.162 0.    0.    0.387 0.    0.    0.254 0.036]\n",
      " [0.    0.029 0.148 0.02  0.    0.    0.296 0.    0.    0.399 0.108]\n",
      " [0.    0.023 0.017 0.046 0.    0.    0.564 0.117 0.005 0.175 0.051]\n",
      " [0.    0.073 0.062 0.165 0.    0.    0.494 0.039 0.    0.14  0.027]\n",
      " [0.    0.    0.038 0.092 0.    0.    0.666 0.004 0.    0.138 0.062]\n",
      " [0.065 0.    0.001 0.057 0.    0.    0.715 0.023 0.    0.091 0.048]\n",
      " [0.    0.    0.219 0.017 0.    0.    0.355 0.129 0.    0.259 0.021]\n",
      " [0.    0.    0.171 0.225 0.    0.    0.28  0.194 0.    0.121 0.009]\n",
      " [0.    0.    0.168 0.025 0.    0.    0.661 0.038 0.    0.106 0.002]]\n"
     ]
    }
   ],
   "source": [
    "print(c.beta_samples.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.15 s\n"
     ]
    }
   ],
   "source": [
    "# Simulated data (Multinomial example)\n",
    "N, L = 200, 100\n",
    "np.random.seed(1)\n",
    "X = np.random.randint(0, 10, (N, L))\n",
    "j = np.random.randint(0, 10, N)\n",
    "\n",
    "c = HDP(gamma=5, f='multinomial', hypers=(L, np.full(L, 1/L)))\n",
    "%time c = c.gibbs_direct(X, j, iters=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Topic Modeling Application\n",
    "\n",
    "Below is an application of the above sampler using a multinomial data model.  The data is `final_project_data.csv`, produced by the modified preprocessing code in this directory, which contains a `(J, L)` matrix in which entry `(j,l)` contains the count of word `l` in document `j`, with the corresponding words given in the column names.\n",
    "\n",
    "For the Dirichlet prior here, we use the observed distribution of the corpus vocabulary over all documents.  Customers could be encoded in four different ways to compare performance:\n",
    "+ As a single word (such that `f` is categorical)\n",
    "+ As a set of all identical words within a given document (each row of the data matrix has one entry, but the value can vary)\n",
    "+ As a set of all words in a single sentence\n",
    "+ As the entire document (essentially making this a non-hierarchical DP)\n",
    "\n",
    "Since this algorithm has not been optimized, only a subset of the full dataframe is used for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv('final_project_data.csv', index_col=0, dtype='int')\n",
    "vocab = full_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_doc(doc_in):\n",
    "    \"\"\"Expands a row (passed in a series) into a dataframe in which each row contains one word.\"\"\"\n",
    "    \n",
    "    doc_in = doc_in.drop(J_ID, axis=1)\n",
    "    doc_in = doc_in.iloc[0, :]\n",
    "    \n",
    "    # Convert from one row per unique word to one row per individual word\n",
    "    each_word = []\n",
    "    while len(doc_in) > 0:\n",
    "        doc_in = doc_in[doc_in > 0]\n",
    "        each_word += list(doc_in.index)\n",
    "        doc_in -= 1\n",
    "        \n",
    "    doc_out = pd.DataFrame(np.zeros((len(each_word), len(vocab)), dtype='int'),\n",
    "                           columns=vocab, dtype='int')\n",
    "    for i, word in enumerate(each_word):\n",
    "        doc_out.loc[i, word] = 1\n",
    "    return doc_out\n",
    "\n",
    "J_ID = 'document#'\n",
    "if J_ID not in full_df.columns:\n",
    "    full_df.insert(0, column=J_ID, value=full_df.index)\n",
    "\n",
    "Jmax = 5\n",
    "wordset_df = full_df.iloc[:Jmax,:].groupby(J_ID).apply(expand_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "ji_indices = wordset_df.index.to_frame()\n",
    "j = np.array(ji_indices[J_ID])\n",
    "X = np.array(wordset_df)\n",
    "# Get the corresponding word each ji is associated with\n",
    "ji_words = vocab[np.where(X > 0)[1]]\n",
    "\n",
    "Tmax, Kmax = np.max(ji_indices[1]), 20\n",
    "# Get a prior distribution over the vocabulary from selected documents\n",
    "L, h_alpha = X.shape[1], np.sum(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.26 s\n"
     ]
    }
   ],
   "source": [
    "%time hdp = HDP(f='categorical', hypers=(L, h_alpha)).gibbs_direct(X, j, 10, Kmax=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given seating assignment in final iteration, cluster customers with same k value\n",
    "k_final = hdp.direct_samples[-1,:,1]\n",
    "clusters = pd.DataFrame({'doc': j, 'word': ji_words, 'cluster': k_final})\n",
    "for k in set(k_final):\n",
    "    clusters_k = clusters[clusters['cluster'] == k].sort_values('word')\n",
    "    print(f\"-----------\\nk = {k} (size {clusters_k.shape[0]})\")\n",
    "    print(clusters_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.5 s\n",
      "Wall time: 20 s\n",
      "Wall time: 3.86 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Unit efficiency tests for direct sampling\n",
    "%time hdp = HDP(f='multinomial', hypers=(L, h_alpha)).gibbs_direct(X, j, 100)\n",
    "%time hdp = HDP(f='multinomial', hypers=(L, h_alpha)).gibbs_direct(csr_matrix(X), j, 100)\n",
    "%time hdp = HDP(f='categorical', hypers=(L, h_alpha)).gibbs_direct(X, j, 100)\n",
    "%time hdp = HDP(f='categorical', hypers=(L, h_alpha)).gibbs_direct(csr_matrix(X), j, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.5 s\n",
      "Wall time: 11.5 s\n",
      "Wall time: 8.49 s\n",
      "Wall time: 3.85 s\n"
     ]
    }
   ],
   "source": [
    "# Unit efficiency tests for CFR sampling vs direct sampling\n",
    "%time hdp = HDP(f='multinomial', hypers=(L, h_alpha)).gibbs_cfr(X, j, 100)\n",
    "%time hdp = HDP(f='multinomial', hypers=(L, h_alpha)).gibbs_direct(X, j, 100)\n",
    "%time hdp = HDP(f='categorical', hypers=(L, h_alpha)).gibbs_cfr(X, j, 100)\n",
    "%time hdp = HDP(f='categorical', hypers=(L, h_alpha)).gibbs_direct(X, j, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.91 s\n",
      "Wall time: 2.6 s\n",
      "Wall time: 34.9 ms\n",
      "Wall time: 2.57 s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "N, L, Jmax, Kmax = 100000, 5000, 50, 100\n",
    "j = np.random.choice(Jmax, size=N)\n",
    "k = np.random.choice(Kmax, size=N)\n",
    "ha = np.ones(L)\n",
    "\n",
    "Xtest = np.zeros((N, L), dtype='int')\n",
    "col_choices = np.random.choice(L, size=N)\n",
    "Xtest[range(N), col_choices] = 1\n",
    "Xtest_sparse = csr_matrix(X)\n",
    "\n",
    "# Unit efficiency tests for mixture components\n",
    "%time fk = mnom_fk_cust(0, Xtest, k, Kmax, L, ha)\n",
    "%time fk = mnom_fk_cust(0, csr_matrix(Xtest), k, Kmax, L, ha)\n",
    "%time fk = cat_fk_cust(0, Xtest, k, Kmax, L, ha)\n",
    "%time fk = cat_fk_cust(0, csr_matrix(Xtest), k, Kmax, L, ha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
