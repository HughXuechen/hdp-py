{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import string\n",
    "from itertools import compress\n",
    "from nltk.corpus import stopwords \n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/tdhopper/topic-modeling-datasets/master/data/raw/Nematode%20biology%20abstracts/cgcbib.txt'\n",
    "file = urllib.request.urlopen(url)\n",
    "data = file.read().decode(\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docsToList(data):\n",
    "    '''\n",
    "    This function takes a string of abstracts and converts it to a list of lists of the words in each abstract.\n",
    "    This function was made specifically for the data obtained here:\n",
    "    https://raw.githubusercontent.com/tdhopper/topic-modeling-datasets/master/data/raw/Nematode%20biology%20abstracts/cgcbib.txt\n",
    "    '''\n",
    "    \n",
    "    # Remove '\\n' and '\\r'\n",
    "    data = data.lower().translate(str.maketrans('\\n', ' '))\n",
    "    data = data.translate(str.maketrans('\\r', ' '))\n",
    "    \n",
    "    # Remove punctuation except for '-' so we can split after each abstract\n",
    "    data = data.translate(str.maketrans('', '', '!\"#$%&\\'()*+,./;<=>?@[\\\\]^_`{|}~'))\n",
    "    \n",
    "    # Remove numbers\n",
    "    data = data.translate(str.maketrans('','', string.digits))\n",
    "    \n",
    "    # Split after 'abstract' is stated\n",
    "    data = data.split('-------------------')\n",
    "    # Remove '-' punctuation now\n",
    "    data = [abstract.translate(str.maketrans('-', ' ')) for abstract in data]\n",
    "    \n",
    "    # Remove entries without the word \"abstract\" in it\n",
    "    abs_check = ['abstract' in i for i in data]\n",
    "    data = list(compress(data, abs_check))\n",
    "\n",
    "    # Only keep the words after 'abstract'\n",
    "    data = [abstract.split('abstract:')[1] for abstract in data]\n",
    "    \n",
    "    # Remove any remaining :'s\n",
    "    data = [abstract.translate(str.maketrans(':', ' ')) for abstract in data]\n",
    "    \n",
    "    # Remove abstracts that only state 'in french'\n",
    "    not_french = ['in french' not in i for i in data]\n",
    "    data = list(compress(data, not_french))\n",
    "    \n",
    "    # Create list of lists output\n",
    "    output = [i.split() for i in data]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducedVocab(lists, stop_words = None, min_word_count = 10):\n",
    "    '''\n",
    "    This function takes a list of words in a list of documents and returns the lists of lists with a reduced\n",
    "    vocabulary, the flattened list, and the vocabulary\n",
    "    '''\n",
    "    \n",
    "    if stop_words == None:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stop words\n",
    "    words = [i for sublist in lists for i in sublist if not i in stop_words]\n",
    "\n",
    "    # Remove words that appear less than min_word_count times\n",
    "    wordSeries = pd.Series(words)\n",
    "    vocab = list(compress(wordSeries.value_counts().index, wordSeries.value_counts() >= min_word_count))\n",
    "    \n",
    "    # Recreate lists with filtered vocab\n",
    "    docs = []\n",
    "    for j in range(len(lists)):\n",
    "        docs.append([i for i in lists[j] if i in vocab])\n",
    "    \n",
    "    #flatten docs\n",
    "    one_list = [i for sublist in docs for i in sublist]\n",
    "    \n",
    "    return docs, one_list, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listsToVec(lists, stop_words = None, min_word_count = 10, verbose = 1):\n",
    "    '''\n",
    "    This function takes a list of lists of the words in each document. It removes any stop words, removes words that\n",
    "    appear 'min_word_count' times or less, and maps each word in the documents' vocabulary to a number. \n",
    "    Returns: data matrix X, where each row is a draw from a categorical distribution representing one word\n",
    "             vector j encoding the corresponding documents each word belongs to'''\n",
    "\n",
    "    # Remove stop words and words that appear less than 'min_word_count' times\n",
    "    docs, one_list, vocab = reducedVocab(lists, stop_words, min_word_count)\n",
    "    \n",
    "    # Map each word to a number\n",
    "    #numbers = list(range(len(vocab)))\n",
    "    #vocab_dict = dict(zip(vocab, numbers))\n",
    "    #x = list(map(vocab_dict.get, one_list))\n",
    "    \n",
    "    # Check for empty lists and print warning if one is found\n",
    "    counter = 0\n",
    "    for i in range(len(docs)-1 ,-1, -1):\n",
    "        if len(docs[i]) == 0:\n",
    "            if verbose > 1:\n",
    "                print(f'WARNING: Document {i} is empty and being removed...')\n",
    "            del docs[i]\n",
    "            counter += 1\n",
    "    \n",
    "    if verbose == 1 and counter > 1:\n",
    "        print(f'WARNING: {counter} documents are empty and being removed...')\n",
    "    \n",
    "    elif verbose == 1 and counter == 1:\n",
    "        print(f'WARNING: {counter} document is empty and being removed...')\n",
    "    \n",
    "    X_matrix = pd.DataFrame(np.zeros((len(one_list), len(vocab))),\n",
    "                           columns=vocab)\n",
    "\n",
    "    for i, word in enumerate(one_list):\n",
    "        X_matrix.loc[i, word] = 1   \n",
    "    \n",
    "    # Determine which document each word belongs to\n",
    "    count, j = 0, []\n",
    "    for i in docs:\n",
    "        j.append([count]*len(i))\n",
    "        count += 1\n",
    "        \n",
    "    # Reduce to a flattened list\n",
    "    j = [i for sublist in j for i in sublist]\n",
    "    \n",
    "    return X_matrix, np.array(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: 5 documents are empty and being removed...\n"
     ]
    }
   ],
   "source": [
    "lists = docsToList(data)\n",
    "x, j = listsToVec(lists[:100], min_word_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7413, 2624)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-968f9157ce7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(np.array(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morris/Quinn\n",
    "\n",
    "What is the etiquette for using other packages inside my functions? Is there something I need to do to ensure the person has them installed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization of reducedVocab funcion\n",
    "\n",
    "Originally the function `reducedVocab` runs on our dataset in Wall time: 3 min 6s\n",
    "\n",
    "Currently the function `reducedVocab` runs in Wall time: 1min 56s\n",
    "\n",
    "Comparatively, gensim's `corpora.Dictionary` runs in 1.17s and the `doc2bow` for text in docs runs in 1.02s.\n",
    "\n",
    "Our function would run in 2s if it did not have to filter the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "%time words = [i for sublist in lists for i in sublist if not i in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old way (1min 23s)\n",
    "%time j = reduce(lambda x, y: x + y, docs, [])\n",
    "\n",
    "# New way (107ms)\n",
    "%time [i for sublist in docs for i in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wordSeries = pd.Series(words)\n",
    "%time vocab = list(compress(wordSeries.value_counts().index, wordSeries.value_counts() >= 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, j = 0, []\n",
    "docs = []\n",
    "\n",
    "# Old way (2min 7s)\n",
    "#%time for j in range(len(lists)): docs.append([i for i in lists[j] if i in vocab])\n",
    "    \n",
    "# New way... map all words to a number, turn into a numpy array, compress it with a mask and convert back to words?\n",
    "dict(zip(set(words), range(len(set(words)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from pprint import pprint\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%time docs, one_list, vocab = reducedVocab(lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time id2word = corpora.Dictionary(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time corpus = [id2word.doc2bow(text) for text in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='gensim.log',\n",
    "                    format=\"%(asctime)s:%(levelname)s:%(message)s\",\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=23,\n",
    "                                           update_every=1,\n",
    "                                           eval_every = 20,\n",
    "                                           chunksize=100,\n",
    "                                           passes=20,\n",
    "                                           alpha='asymmetric', # 1D array of length equal to number of expected topics - expresses a-priori belief for each topics prob\n",
    "                                           #eta = , # a-prior belief on word probability\n",
    "                                           per_word_topics=True,\n",
    "                                           iterations = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "p = re.compile(\"(-*\\d+\\.\\d+) per-word .* (\\d+\\.\\d+) perplexity\")\n",
    "matches = [p.findall(l) for l in open('gensim.log')]\n",
    "matches = [m for m in matches if len(m) > 0]\n",
    "tuples = [t[0] for t in matches]\n",
    "perplexity = [float(t[1]) for t in tuples]\n",
    "liklihood = [float(t[0]) for t in tuples]\n",
    "iter = list(range(0,len(tuples)*10,10))\n",
    "plt.plot(iter,liklihood,c=\"black\")\n",
    "plt.ylabel(\"log liklihood\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.title(\"Topic Model Convergence\")\n",
    "plt.grid()\n",
    "plt.savefig(\"convergence_liklihood.pdf\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has lots of problems... \n",
    "- The log perplexity method doesn't return 'perplexity', need to calculate that on my own. \n",
    "- Need to implement 10-fold cross-validation\n",
    "- Mixture component cardinalities ranging from 10 to 120\n",
    "- Need to figure out how to incorporate the symmetric Dirichlet distribution with parameters \n",
    "    of .5 for the prior H over topic distributions\n",
    "- Distribution over topics in LDA was assumed to be symmetric Dirichlet w \n",
    "    parameters $\\alpha_0/L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.log_perplexity()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
