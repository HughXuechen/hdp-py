{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib\n",
    "import string\n",
    "from itertools import compress\n",
    "from nltk.corpus import stopwords \n",
    "from functools import reduce\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducedVocab(lists, stop_words = None, min_word_count = 10):\n",
    "    '''This function takes a list of words in a list of documents and returns the lists of lists with a reduced\n",
    "       vocabulary, the flattened list, and the vocabulary'''\n",
    "    \n",
    "    if stop_words == None:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stop words\n",
    "    words = [i for sublist in lists for i in sublist if not i in stop_words]\n",
    "\n",
    "    # Remove words that appear less than min_word_count times\n",
    "    wordSeries = pd.Series(words)\n",
    "    vocab = list(compress(wordSeries.value_counts().index, wordSeries.value_counts() >= min_word_count))\n",
    "    \n",
    "    # Recreate lists with filtered vocab\n",
    "    docs = []\n",
    "    for j in range(len(lists)):\n",
    "        docs.append([i for i in lists[j] if i in vocab])\n",
    "    \n",
    "    #flatten docs\n",
    "    one_list = [i for sublist in docs for i in sublist]\n",
    "    \n",
    "    return docs, one_list, vocab\n",
    "\n",
    "def listsToVec(lists, stop_words = None, min_word_count = 10, verbose = 1):\n",
    "    '''\n",
    "    This function takes a list of lists of the words in each document. It removes any stop words, removes words that\n",
    "    appear 'min_word_count' times or less, and maps each word in the documents' vocabulary to a number. \n",
    "    Two flattened vectors are returned, the mapped numbers 'x', and the corresponding document each word belongs to 'j'.'''\n",
    "\n",
    "    # Remove stop words and words that appear less than 'min_word_count' times\n",
    "    docs, one_list, vocab = reducedVocab(lists, stop_words, min_word_count)\n",
    "    \n",
    "    # Map each word to a number\n",
    "    #numbers = list(range(len(vocab)))\n",
    "    #vocab_dict = dict(zip(vocab, numbers))\n",
    "    #x = list(map(vocab_dict.get, one_list))\n",
    "    \n",
    "    # Check for empty lists and print warning if one is found\n",
    "    counter = 0\n",
    "    for i in range(len(docs)-1 ,-1, -1):\n",
    "        if len(docs[i]) == 0:\n",
    "            if verbose > 1:\n",
    "                print(f'WARNING: Document {i} is empty and being removed...')\n",
    "            del docs[i]\n",
    "            counter += 1\n",
    "    \n",
    "    if verbose == 1 and counter > 1:\n",
    "        print(f'WARNING: {counter} documents are empty and being removed...')\n",
    "    \n",
    "    elif verbose == 1 and counter == 1:\n",
    "        print(f'WARNING: {counter} document is empty and being removed...')\n",
    "    \n",
    "    X_matrix = pd.DataFrame(np.zeros((len(one_list), len(vocab))),\n",
    "                           columns=vocab)\n",
    "\n",
    "    for i, word in enumerate(one_list):\n",
    "        X_matrix.loc[i, word] = 1   \n",
    "    \n",
    "    # Determine which document each word belongs to\n",
    "    count, j = 0, []\n",
    "    for i in docs:\n",
    "        j.append([count]*len(i))\n",
    "        count += 1\n",
    "        \n",
    "    # Reduce to a flattened list\n",
    "    j = [i for sublist in j for i in sublist]\n",
    "    \n",
    "    return X_matrix, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-718fe8b63736>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-18-718fe8b63736>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    f = open(f'{directory.decode('ascii')}/{filename}', 'r')\u001b[0m\n\u001b[1;37m                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "directory = os.fsencode('../data')\n",
    "docs = []\n",
    "for file in os.listdir(directory):\n",
    "    root = directory.decode('ascii')\n",
    "    filename = os.fsdecode(file)\n",
    "    f = open(f'{root}/{filename}', 'r')\n",
    "    data= f.read()\n",
    "    soup = BeautifulSoup(data)\n",
    "    contents = soup.findAll('text')\n",
    "    f.close()\n",
    "    docs.append(str(contents).split('</text>'))\n",
    "\n",
    "docs = [i for doc in docs for i in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split on </dateline> and keep everything after it\n",
    "docs = list(compress(docs, ['</dateline>' in i for i in docs]))\n",
    "docs = [i.split('</dateline>')[1] for i in docs]\n",
    "docs = [i.lower().translate(str.maketrans('\\n', ' ')) for i in docs]\n",
    "docs = [i.translate(str.maketrans('\\r', ' ')) for i in docs]\n",
    "docs = [i.translate(str.maketrans('\\x03', ' ')) for i in docs]\n",
    "docs = [i.translate(str.maketrans('', '', string.punctuation)) for i in docs]\n",
    "docs = [i.translate(str.maketrans('', '', string.digits)) for i in docs]\n",
    "docs = [i.replace('said',' ') for i in docs] # another stop word\n",
    "docs = [i.replace('reuter', ' ') for i in docs] # the name of the company at the end of most articles\n",
    "docs = [i.split() for i in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory.decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'../data/'\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{directory}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
