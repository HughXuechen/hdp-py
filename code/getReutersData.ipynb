{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib\n",
    "import string\n",
    "from itertools import compress\n",
    "from nltk.corpus import stopwords \n",
    "from functools import reduce\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docsToList(data):\n",
    "    '''\n",
    "    This function takes a string of abstracts and converts it to a list of lists of the words in each abstract.\n",
    "    This function was made specifically for the data obtained here:\n",
    "    https://raw.githubusercontent.com/tdhopper/topic-modeling-datasets/master/data/raw/Nematode%20biology%20abstracts/cgcbib.txt\n",
    "    '''\n",
    "    \n",
    "    # Remove '\\n' and '\\r'\n",
    "    data = data.lower().translate(str.maketrans('\\n', ' '))\n",
    "    data = data.translate(str.maketrans('\\r', ' '))\n",
    "    \n",
    "    # Remove punctuation except for '-' so we can split after each abstract\n",
    "    data = data.translate(str.maketrans('', '', '!\"#$%&\\'()*+,./;<=>?@[\\\\]^_`{|}~'))\n",
    "    \n",
    "    # Remove numbers\n",
    "    data = data.translate(str.maketrans('','', string.digits))\n",
    "    \n",
    "    # Split after 'abstract' is stated\n",
    "    data = data.split('-------------------')\n",
    "    # Remove '-' punctuation now\n",
    "    data = [abstract.translate(str.maketrans('-', ' ')) for abstract in data]\n",
    "    \n",
    "    # Remove entries without the word \"abstract\" in it\n",
    "    abs_check = ['abstract' in i for i in data]\n",
    "    data = list(compress(data, abs_check))\n",
    "\n",
    "    # Only keep the words after 'abstract'\n",
    "    data = [abstract.split('abstract:')[1] for abstract in data]\n",
    "    \n",
    "    # Remove any remaining :'s\n",
    "    data = [abstract.translate(str.maketrans(':', ' ')) for abstract in data]\n",
    "    \n",
    "    # Remove abstracts that only state 'in french'\n",
    "    not_french = ['in french' not in i for i in data]\n",
    "    data = list(compress(data, not_french))\n",
    "    \n",
    "    # Create list of lists output\n",
    "    output = [i.split() for i in data]\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def reducedVocab(lists, stop_words = None, min_word_count = 10):\n",
    "    '''\n",
    "    This function takes a list of words in a list of documents and returns the lists of lists with a reduced\n",
    "    vocabulary, the flattened list, and the vocabulary\n",
    "    '''\n",
    "    \n",
    "    if stop_words == None:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stop words\n",
    "    words = [i for sublist in lists for i in sublist if not i in stop_words]\n",
    "\n",
    "    # Remove words that appear less than min_word_count times\n",
    "    wordSeries = pd.Series(words)\n",
    "    vocab = list(compress(wordSeries.value_counts().index, wordSeries.value_counts() >= min_word_count))\n",
    "    \n",
    "    # Recreate lists with filtered vocab\n",
    "    docs = []\n",
    "    for j in range(len(lists)):\n",
    "        docs.append([i for i in lists[j] if i in vocab])\n",
    "    \n",
    "    #flatten docs\n",
    "    one_list = [i for sublist in docs for i in sublist]\n",
    "    \n",
    "    return docs, one_list, vocab\n",
    "\n",
    "\n",
    "def listsToVec(lists, stop_words = None, min_word_count = 10, verbose = 1):\n",
    "    '''\n",
    "    This function takes a list of lists of the words in each document. It removes any stop words, removes words that\n",
    "    appear 'min_word_count' times or less, and maps each word in the documents' vocabulary to a number. \n",
    "    Returns: data matrix X, where each row is a draw from a categorical distribution representing one word\n",
    "             vector j encoding the corresponding documents each word belongs to'''\n",
    "\n",
    "    # Remove stop words and words that appear less than 'min_word_count' times\n",
    "    docs, one_list, vocab = reducedVocab(lists, stop_words, min_word_count)\n",
    "    \n",
    "    # Map each word to a number\n",
    "    #numbers = list(range(len(vocab)))\n",
    "    #vocab_dict = dict(zip(vocab, numbers))\n",
    "    #x = list(map(vocab_dict.get, one_list))\n",
    "    \n",
    "    # Check for empty lists and print warning if one is found\n",
    "    counter = 0\n",
    "    for i in range(len(docs)-1 ,-1, -1):\n",
    "        if len(docs[i]) == 0:\n",
    "            if verbose > 1:\n",
    "                print(f'WARNING: Document {i} is empty and being removed...')\n",
    "            del docs[i]\n",
    "            counter += 1\n",
    "    \n",
    "    if verbose == 1 and counter > 1:\n",
    "        print(f'WARNING: {counter} documents are empty and being removed...')\n",
    "    \n",
    "    elif verbose == 1 and counter == 1:\n",
    "        print(f'WARNING: {counter} document is empty and being removed...')\n",
    "    \n",
    "    X_matrix = pd.DataFrame(np.zeros((len(one_list), len(vocab))),\n",
    "                           columns=vocab)\n",
    "\n",
    "    for i, word in enumerate(one_list):\n",
    "        X_matrix.loc[i, word] = 1   \n",
    "    \n",
    "    # Determine which document each word belongs to\n",
    "    count, j = 0, []\n",
    "    for i in docs:\n",
    "        j.append([count]*len(i))\n",
    "        count += 1\n",
    "        \n",
    "    # Reduce to a flattened list\n",
    "    j = [i for sublist in j for i in sublist]\n",
    "    \n",
    "    return X_matrix.astype('int'), np.array(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.fsencode('../data')\n",
    "docs = []\n",
    "for file in os.listdir(directory):\n",
    "    root = directory.decode('ascii')\n",
    "    filename = os.fsdecode(file)\n",
    "    f = open(f'{root}/{filename}', 'r')\n",
    "    data= f.read()\n",
    "    soup = BeautifulSoup(data)\n",
    "    contents = soup.findAll('text')\n",
    "    f.close()\n",
    "    docs.append(str(contents).split('</text>'))\n",
    "\n",
    "docs = [i for doc in docs for i in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split on </dateline> and keep everything after it\n",
    "docs = list(compress(docs, ['</dateline>' in i for i in docs]))\n",
    "docs = [i.split('</dateline>')[1] for i in docs]\n",
    "docs = [i.lower().translate(str.maketrans('\\n', ' ')) for i in docs]\n",
    "docs = [i.translate(str.maketrans('\\r', ' ')) for i in docs]\n",
    "docs = [i.translate(str.maketrans('\\x03', ' ')) for i in docs]\n",
    "docs = [i.translate(str.maketrans('', '', string.punctuation)) for i in docs]\n",
    "docs = [i.translate(str.maketrans('', '', string.digits)) for i in docs]\n",
    "docs = [i.replace('said',' ') for i in docs] # another stop word\n",
    "docs = [i.replace('reuter', ' ') for i in docs] # the name of the company at the end of most articles\n",
    "docs = [i.split() for i in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
