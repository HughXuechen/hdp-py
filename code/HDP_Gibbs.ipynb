{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDP Gibbs Samplers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *( 5.1 )* Posterior Sampling in the Chinese Restaurant Franchise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Overview\n",
    "\n",
    "The Hierarchical Dirichlet Process mixture model is given by\n",
    "$$\\begin{aligned}\n",
    "G_0 | \\gamma, H &\\sim DP(\\gamma, H) \\\\\n",
    "G_j | \\alpha_0, G_0 &\\sim DP(\\alpha_0, G_0) \\\\\n",
    "\\theta_{ji} | G_j &\\sim G_j \\\\\n",
    "x_{ji} | \\theta_{ji} &\\sim F(\\theta_{ji})\n",
    "\\end{aligned} $$\n",
    "\n",
    "This model is able to non-parametrically cluster each group's data while sharing information both between and within groups.  A Dirichlet process is essentially a discrete distribution with atoms drawn from a (not-necessarily discrete) base measure $H$ and gradually decreasing weights determined by the \"stick-breaking process.\"  In the HDP, each group is a Dirichlet process drawn from another DP $G_0$, so these will contain the same atoms as $G_0$ but with different weights:\n",
    "$$\\begin{aligned}\n",
    "G_0 &= \\sum_{k=1}^{\\infty} \\beta_k \\delta(\\phi_k) \\\\\n",
    "G_j &= \\sum_{k=1}^{\\infty} \\pi_{jk} \\delta(\\phi_k) \\\\\n",
    "\\phi_k | H &\\sim H\n",
    "\\end{aligned} $$\n",
    "Additionally, if we define $\\beta, \\pi_j$ as the collected weights above, it can be shown that these vectors encode a distribution over $\\mathbb{Z}^+$ such that $\\beta | \\gamma \\sim GEM(\\gamma)$ and $\\pi_j | \\alpha_0, \\beta \\sim DP(\\alpha_0, \\beta)$.\n",
    "\n",
    "Successive draws from a DP exhibit clustering behavior, since the probability of taking a certain value is a related to the number of previous draws of that value.  This is shown in the hierarchical sense by the *Chinese restaurant franchise* process.  Imagine a group of Chinese restaurants with a certain number of tables at each restaurant.  Let $\\phi_k$ be the global dishes, drawn from $H$; $\\psi_{jt}$ be the table-specific dishes, drawn from $G_0$; and $\\theta_{ji}$ be the customer-specific dishes, drawn from $G_j$.  Denote $z_{ji}$ as the dish index eaten by customer $ji$; $t_{ji}$ as the table index where customer $ji$ sits; $k_{jt}$ be the dish index served at table $jt$; $n_{jtk}$ be the customer counts; and $m_{jk}$ be the table counts.  Then:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\theta_{ji} | \\text{other } \\theta, \\alpha_0, G_0 &\\sim\n",
    "    \\sum_{t=1}^{m_{j\\cdot}} \\frac{n_{jt\\cdot}}{i-1+\\alpha_0} \\delta(\\psi_{jt}) +\n",
    "                            \\frac{\\alpha_0}{i-1+\\alpha_0} G_0 \\\\\n",
    "\\psi_{jt} | \\text{other } \\psi, \\gamma, H &\\sim\n",
    "    \\sum_{k=1}^{K} \\frac{m_{\\cdot k}}{m_{\\cdot k} + \\gamma} \\delta(\\phi_k) +\n",
    "                            \\frac{\\gamma}{m_{\\cdot k} + \\gamma} H\n",
    "\\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Conditionals\n",
    "\n",
    "Choose some base measure $h(\\cdot)$ and a conjugate data-generating distribution $f(\\cdot | \\theta)$.  Important to compute are $f_k^{-x_{ji}}(x_{ji})$, the mixture component of customer $ij$ under $k$, and $f_k^{-\\mathbf{x}_{jt}}(\\mathbf{x}_{jt})$, the mixture component of table $jt$ under $k$.  This is done by integrating out $\\phi_k$ over the joint density of all such points, for example:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "f_k^{-x_{ji}}(x_{ji}) &= \\frac { \\int f(x_{ij} | \\phi_k) g(k)d\\phi_k } { \\int g(k)d\\phi_k } \\\\\n",
    "g(k) &= h(\\phi_k) \\prod_{j'i' \\neq ji, z_{j'i'} = k} f(x_{j'i'} | \\phi_k) \n",
    "\\end{aligned} $$\n",
    "\n",
    "The corresponding mixture components for a new customer assignment and new table assignment are denoted $f_{k^*}^{-x_{ji}}(x_{ji})$ and $f_{k^*}^{-\\mathbf{x}_{jt}}(\\mathbf{x}_{jt})$, which are special cases of their the respective $f_k$ component where no data points have $z_{ij} = k^*$.\n",
    "\n",
    "Using this, we first compute the likelihood of a given point $x_{ji}$ given the current clustering scheme:\n",
    "$$\n",
    "p(x_{ji} | t^{-ji}, t_{ji} = t^*, k) =\n",
    "    \\sum_{k=1}^{K} \\frac{m_{\\cdot k}}{m_{\\cdot k} + \\gamma} f_k^{-x_{ji}}(x_{ji}) +\n",
    "                            \\frac{\\gamma}{m_{\\cdot k} + \\gamma} f_{k^*}^{-x_{ji}}(x_{ji})\n",
    "$$\n",
    "\n",
    "For efficiency, the Gibbs scheme implemented below only samples the $t$ and $k$ indexes (which can later be reverse-engineered to obtain the actual parameters).  The state space of the $k$ values is technically infinite, and the number of tables/dishes currently associated with the data is undefined.  We keep a running list of active $t$ and $k$ values.  Each update step, each customer is assigned either to one of the existing tables or to a new table, and if a customer is assigned to a new table, a new $k$ corresponding value gets drawn; similarly, each table is assigned a dish, either from the existing dishes or with a new dish.  If a table/dish becomes unrepresented in the current scheme, it gets removed from its respective list.  The update full conditionals are:\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "p(t_{ji} = t | t^{-ji}, k, ...) &\\propto \\begin{cases}\n",
    "    n_{jt\\cdot}^{-ji} f_{k_{jt}}^{-x_{ji}}(x_{ji}) & t\\text{ used}\\\\\n",
    "    \\alpha_0 p(x_{ji} | ...) & t\\text{ new}\n",
    "    \\end{cases} \\\\\n",
    "p(k_{jt} = k | t, k^{-jt}) &\\propto \\begin{cases}\n",
    "    m_{\\cdot k} f_k^{-\\mathbf{x}_{jt}}(\\mathbf{x}_{jt}) & k\\text{ used}\\\\\n",
    "    \\gamma f_{k^*}^{-\\mathbf{x}_{jt}}(\\mathbf{x}_{jt}) & k\\text{ new}\n",
    "    \\end{cases} \\\\\n",
    "\\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution-Specific Mixture Components\n",
    "\n",
    "The only part of this sampling algorithm that depends on the choice of the measures $H$ and $F$ are the mixture components $f_k$, so this is the only part that needs rewritten for each type of model.  Let\n",
    "$$ \\begin{aligned}\n",
    "V_{kji} &= \\{ j'i' : j'i' \\neq ji, z_{j'i'} = k \\} \\\\\n",
    "W_{kjt} &= \\{ j'i' : j't_{j'i'} \\neq jt, k_{j't_{j'i'} = k} \\} \\\\\n",
    "T_{jt} &= \\{ j'i': t_{j'i'} = jt \\} \\\\\n",
    "\\end{aligned} $$\n",
    "$V$ is the set of all customers (excluding customer $ij$) eating dish $k$; $W$ is the set of all customers at tables (excluding table $jt$) eating $k$; these correspond to the product terms in the mixture components.  By conjugacy rules and kernel tricks, each $f_k$ can be expressed as functions of these sets.  Each $f_{k^*}$ can be found by using the corresponding $f_k$ formula where $V$ or $W$ is the empty set.\n",
    "\n",
    "*F = Poisson, H = Gamma*\n",
    "$$ \\begin{aligned}\n",
    "f_k^{-x_{ji}}(x_{ji}) &= \\frac{1}{x_{ji}!} \\cdot\n",
    "    \\frac{\\Gamma(x_{ji} + \\alpha_v)}{(1 + \\beta_v)^{x_{ji} + \\alpha_v}} \\cdot\n",
    "    \\frac{(\\beta_v)^{\\alpha_v}}{\\Gamma(\\alpha_v)} \\\\\n",
    "f_k^{-\\mathbf{x}_{jt}}(\\mathbf{x}_{jt}) &= \\frac{1}{\\prod_T x_t!} \\cdot\n",
    "    \\frac{\\Gamma(\\sum_T x_t + \\alpha_w)}{(1 + \\beta_w)^{\\sum_T x_t + \\alpha_w}} \\cdot\n",
    "    \\frac{(\\beta_w)^{\\alpha_w}}{\\Gamma(\\alpha_w)} \\\\\n",
    "\\alpha_v &= \\sum_V x_v + \\alpha \\quad , \\quad \\beta_v = |V| + \\beta \\\\\n",
    "\\alpha_w &= \\sum_W x_w + \\alpha \\quad , \\quad \\beta_w = |W| + \\beta \\\\\n",
    "\\end{aligned} $$\n",
    "\n",
    "*F = Normal (known variance), H = Normal*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import loggamma\n",
    "\n",
    "def pois_fk(x, t, k, Kmax, ha, hb):\n",
    "    \"\"\"\n",
    "    Computes in one sweep the mixture components for each customer/table for each k.\n",
    "    MODEL: base measure H ~ Gamma(ha, hb), F(x|phi) ~ Poisson(phi)\n",
    "    All components are calculated exactly in log-space and then exponentiated.\n",
    "    \n",
    "    returns: fk_cust = (N, Kmax) matrix\n",
    "             fk_tabl = (T, Kmax) matrix\n",
    "             fknew_cust = (N,) vector\n",
    "             fknew_tabl = (T,) vector\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x.flatten()  # reshape to 1D, since gibbs routine passes in a 2D array\n",
    "    fk_cust = np.zeros((len(k), Kmax))\n",
    "    fknew_cust = np.zeros(len(k))\n",
    "    \n",
    "    # FOR k WITH NO MEMBERS\n",
    "    \n",
    "    # FOR k WITH MEMBERS\n",
    "    for kk in range(Kmax):\n",
    "        x_kk = x[k == kk]               # subset of x values with value kk\n",
    "        x_in = (k == kk).astype('int')  # offset for x values in subset\n",
    "        # If a value for k is not used, same as the prior\n",
    "        if len(x_kk) == 0: continue\n",
    "        \n",
    "        # Compute (a,b) params from gamma kernel tricks done in fk function\n",
    "        a_denom = (np.sum(x_kk) - x_in*x) + ha\n",
    "        b_denom = (len(x_kk) - x_in) + hb\n",
    "        a_numer = x + a_denom\n",
    "        b_numer = 1 + b_denom\n",
    "        #print(f\"kk = {kk}, subset size = {len(x_kk)}\")\n",
    "        #print(f\"{np.c_[x,k,a_numer,a_denom,b_numer,b_denom]}\")\n",
    "        log_fk_cust = ( -loggamma(x + 1) + loggamma(a_numer) - loggamma(a_denom) -\n",
    "                        a_numer * np.log(b_numer) + a_denom * np.log(b_denom) )\n",
    "        fk_cust[:, kk] = np.exp(log_fk_cust)\n",
    "    \n",
    "    return fk_cust      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFRP:\n",
    "    \"\"\"\n",
    "    Model implementing the Chinese Franchise Restaurant Process.\n",
    "    \n",
    "    CONSTRUCTOR PARAMETERS\n",
    "    - gamma, alpha0: scaling parameters > 0 for base measures H and G0\n",
    "    - f: string representing distribution of data; h is chosen to be conjugate\n",
    "    - hypers: tuple of hyperparameter values specific to f/h scheme chosen\n",
    "    \n",
    "    PRIVATE ATTRIBUTES (volatile)\n",
    "    - t_: set of active t values; formatted as {j: set(t...), ...}\n",
    "    - k_: set of active k values\n",
    "    - tk_: (J x Tmax) of corresponding k values for each t\n",
    "    - n_: (J x Tmax) tensor specifying counts of customers\n",
    "    - m_: (J x Kmax) matrix specifying counts of tables\n",
    "    - f_, h_: distribution functions\n",
    "    - fk_routine_: a function to compute mixing components for Gibbs sampling\n",
    "    \n",
    "    PUBLIC ATTRIBUTES\n",
    "    post_samples: (S x 4) matrix of (j, t, k, phi) values for each data point i;\n",
    "                  exists only after gibbs() has been called\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=1, alpha0=1, f='poisson', hypers=None):\n",
    "        self.g_ = gamma\n",
    "        self.a0_ = alpha0\n",
    "        self.set_priors(f, hypers)\n",
    "        \n",
    "    def set_priors(self, f, hypers):\n",
    "        \"\"\"\n",
    "        Initializes the type of base measure h_ and data-generation function f_.\n",
    "        Also sets hypers_, the relevelant hyperparameters and\n",
    "                  fk_routine_, the function to compute mixing components.\n",
    "        \"\"\"\n",
    "        if f == 'poisson':\n",
    "            # Specify parameters of H ~ Gamma(a,b)\n",
    "            if hypers is None:\n",
    "                self.hypers_ = (1,1)\n",
    "            else: self.hypers_ = hypers\n",
    "            self.fk_routine_ = pois_fk\n",
    "        \n",
    "        \n",
    "    def gibbs(self, x, j, iters=1, Kmax=10):\n",
    "        \"\"\"\n",
    "        Runs the Gibbs sampler to generate posterior estimates of t and k.\n",
    "        x: data matrix, stored row-wise if multidimensional\n",
    "        j: vector of group labels; must have same #rows as x\n",
    "        iters: number of iterations to run\n",
    "        Kmax: maximum number of atoms to draw from base measure H\n",
    "        \n",
    "        results: creation of post_samples attribute\n",
    "        \"\"\"\n",
    "            \n",
    "        group_counts = pd.Series(j).value_counts()\n",
    "        # number of tables cannot exceed size of max group\n",
    "        J, Tmax, N = np.max(j), np.max(group_counts), len(j)\n",
    "        self.n_ = np.zeros((J, Tmax))\n",
    "        self.m_ = np.zeros((J, Kmax))\n",
    "        self.post_samples = np.zeros((iters+1, N, 4), dtype='int')\n",
    "        self.post_samples[:,:,0] = j\n",
    "        self.t_, self.k_ = {}, set()\n",
    "        self.tk_ = np.zeros((J, Tmax))\n",
    "        \n",
    "        # Set random initial values for t and k assignments\n",
    "        t0, k0 = self.post_samples[0,:,1], self.post_samples[0,:,2]   # define two views\n",
    "        t0[:] = np.random.randint(1, Tmax, size=N)\n",
    "        self.t_ = {jj: set(t0[j == jj]) for jj in range(J)}\n",
    "        self.tk_ = np.random.randint(1, Kmax//2, (J, Tmax))           # one table => one dish\n",
    "        for jj in range(J):\n",
    "            for tt in self.t_[jj]:\n",
    "                k0[np.logical_and(j == jj, t0 == tt)] = self.tk_[jj, tt]\n",
    "        \n",
    "        for s in range(iters):\n",
    "            t_prev, k_prev = self.post_samples[s,:,1], self.post_samples[s,:,2]\n",
    "            t_next, k_next = self.post_samples[s+1,:,1], self.post_samples[s+1,:,2]\n",
    "            \n",
    "            # Get a matrix of log fk(x_ji) values (dependent on model specification)\n",
    "            fk = self.fk_routine_(x, t_prev, k_prev, Kmax, *self.hypers_) \n",
    "            self.fk_ = fk\n",
    "            \n",
    "            t_next = t_prev\n",
    "            # Cycle through each t value of each customer, conditioning on everything\n",
    "            # Randomize the order in which updates occur\n",
    "            for i in np.random.permutation(N):\n",
    "                continue\n",
    "                \n",
    "            k_next = k_prev\n",
    "            # Similarly, cycle through the k values of each table\n",
    "            for i in np.random.permutation(N):\n",
    "                continue            \n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated data\n",
    "N = 25\n",
    "np.random.seed(0)\n",
    "j = np.random.randint(1, 10, N)\n",
    "x = np.random.poisson(j, N)\n",
    "data = np.c_[x, j]\n",
    "\n",
    "c = CFRP(hypers=(1,10)).gibbs(x[:,None], j)\n",
    "iter0 = c.post_samples[0,:,:]\n",
    "#print(c.fk_)\n",
    "#print(np.c_[c.post_samples[0,:,2], np.argmax(c.fk_, axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.      1.     -8.761  -9.369 -11.984 -25.023 -10.928]\n",
      " [  2.      4.     -1.65   -1.818  -1.178  -1.966  -1.339]\n",
      " [  2.      1.     -1.557  -1.523  -1.935  -6.242  -1.743]\n",
      " [  1.      1.     -1.413  -1.489  -1.375  -4.045  -1.337]\n",
      " [  1.      1.     -8.761  -9.369 -11.984 -25.023 -10.928]\n",
      " [  2.      1.     -1.557  -1.523  -1.935  -6.242  -1.743]\n",
      " [  2.      4.     -7.505  -7.042 -10.413 -22.625 -11.538]\n",
      " [  3.      3.     -1.65   -1.818  -1.178  -2.493  -1.307]\n",
      " [  4.      2.     -3.328  -3.059  -5.769 -13.138  -4.28 ]\n",
      " [  4.      1.     -5.224  -5.279  -7.471 -17.856  -6.671]\n",
      " [  2.      4.     -4.221  -3.891  -6.119 -15.489  -6.239]\n",
      " [  4.      0.     -9.038  -7.042 -10.413 -22.625  -9.435]\n",
      " [  4.      0.    -16.594 -12.359 -17.012 -32.255 -15.764]\n",
      " [  4.      1.     -1.557  -1.523  -1.935  -6.242  -1.743]\n",
      " [  3.      2.     -5.224  -4.842  -9.417 -17.856  -6.671]\n",
      " [  3.      4.     -3.328  -3.059  -4.867 -13.138  -4.804]\n",
      " [  2.      1.     -3.328  -3.222  -4.867 -13.138  -4.28 ]\n",
      " [  3.      0.     -4.759  -3.891  -6.119 -15.489  -5.422]\n",
      " [  4.      1.     -1.65   -1.878  -1.178  -1.966  -1.307]\n",
      " [  3.      1.     -1.962  -1.862  -2.741  -8.504  -2.407]\n",
      " [  1.      0.     -2.727  -2.366  -3.732 -10.807  -3.266]\n",
      " [  4.      2.     -4.221  -3.891  -7.492 -15.489  -5.422]\n",
      " [  4.      4.     -4.221  -3.891  -6.119 -15.489  -6.239]\n",
      " [  2.      4.     -2.553  -2.838  -1.63   -0.174  -2.065]\n",
      " [  2.      1.     -5.224  -5.279  -7.471 -17.856  -6.671]]\n"
     ]
    }
   ],
   "source": [
    "# t, k, log(customer mixtures)\n",
    "print(np.c_[iter0[:,1:3], np.log(c.fk_[:,:5]).round(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *( 5.2 )* Posterior Sampling with Augmented Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *( 5.3 )* Posterior Sampling by Direct Assignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
