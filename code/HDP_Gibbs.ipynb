{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDP Gibbs Samplers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *( 5.1 )* Posterior Sampling in the Chinese Restaurant Franchise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Overview\n",
    "\n",
    "The Hierarchical Dirichlet Process mixture model is given by\n",
    "$$\\begin{aligned}\n",
    "G_0 | \\gamma, H &\\sim DP(\\gamma, H) \\\\\n",
    "G_j | \\alpha_0, G_0 &\\sim DP(\\alpha_0, G_0) \\\\\n",
    "\\theta_{ji} | G_j &\\sim G_j \\\\\n",
    "x_{ji} | \\theta_{ji} &\\sim F(\\theta_{ji})\n",
    "\\end{aligned} $$\n",
    "\n",
    "This model is able to non-parametrically cluster each group's data while sharing information both between and within groups.  A Dirichlet process is essentially a discrete distribution with atoms drawn from a (not-necessarily discrete) base measure $H$ and gradually decreasing weights determined by the \"stick-breaking process.\"  In the HDP, each group is a Dirichlet process drawn from another DP $G_0$, so these will contain the same atoms as $G_0$ but with different weights:\n",
    "$$\\begin{aligned}\n",
    "G_0 &= \\sum_{k=1}^{\\infty} \\beta_k \\delta(\\phi_k) \\\\\n",
    "G_j &= \\sum_{k=1}^{\\infty} \\pi_{jk} \\delta(\\phi_k) \\\\\n",
    "\\phi_k | H &\\sim H\n",
    "\\end{aligned} $$\n",
    "Additionally, if we define $\\beta, \\pi_j$ as the collected weights above, it can be shown that these vectors encode a distribution over $\\mathbb{Z}^+$ such that $\\beta | \\gamma \\sim GEM(\\gamma)$ and $\\pi_j | \\alpha_0, \\beta \\sim DP(\\alpha_0, \\beta)$.\n",
    "\n",
    "Successive draws from a DP exhibit clustering behavior, since the probability of taking a certain value is a related to the number of previous draws of that value.  This is shown in the hierarchical sense by the *Chinese restaurant franchise* process.  Imagine a group of Chinese restaurants with a certain number of tables at each restaurant.  Let $\\phi_k$ be the global dishes, drawn from $H$; $\\psi_{jt}$ be the table-specific dishes, drawn from $G_0$; and $\\theta_{ji}$ be the customer-specific dishes, drawn from $G_j$.  Denote $z_{ji}$ as the dish index eaten by customer $ji$; $t_{ji}$ as the table index where customer $ji$ sits; $k_{jt}$ be the dish index served at table $jt$; $n_{jtk}$ be the customer counts; and $m_{jk}$ be the table counts.  Then:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\theta_{ji} | \\text{other } \\theta, \\alpha_0, G_0 &\\sim\n",
    "    \\sum_{t=1}^{m_{j\\cdot}} \\frac{n_{jt\\cdot}}{i-1+\\alpha_0} \\delta(\\psi_{jt}) +\n",
    "                            \\frac{\\alpha_0}{i-1+\\alpha_0} G_0 \\\\\n",
    "\\psi_{jt} | \\text{other } \\psi, \\gamma, H &\\sim\n",
    "    \\sum_{k=1}^{K} \\frac{m_{\\cdot k}}{m_{\\cdot k} + \\gamma} \\delta(\\phi_k) +\n",
    "                            \\frac{\\gamma}{m_{\\cdot k} + \\gamma} H\n",
    "\\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Conditionals\n",
    "\n",
    "Choose some base measure $h(\\cdot)$ and a conjugate data-generating distribution $f(\\cdot | \\theta)$.  Important to compute is $f_k^{-x_{ji}}(x_{ji})$, the conditional density of a point $x_{ji}$ with mixture component $k$, after integrating out $\\phi_k$ over the joint density of all such points:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "f_k^{-x_{ji}}(x_{ji}) &= \\frac { \\int f(x_{ij} | \\phi_k) g(k)d\\phi_k } { \\int g(k)d\\phi_k } \\\\\n",
    "g(k) &= \\prod_{j'i' \\neq ji, z_{j'i'} = k} f(x_{j'i'} | \\phi_k) h(\\phi_k)\n",
    "\\end{aligned} $$\n",
    "\n",
    "Using this, we first compute the likelihood of a given point $x_{ji}$ given the current clusering scheme:\n",
    "$$\n",
    "p(x_{ji} | t^{-ji}, t_{ji} = t^*, k) =\n",
    "    \\sum_{k=1}^{K} \\frac{m_{\\cdot k}}{m_{\\cdot k} + \\gamma} f_k^{-x_{ji}}(x_{ji}) +\n",
    "                            \\frac{\\gamma}{m_{\\cdot k} + \\gamma} f_{k^*}^{-x_{ji}}(x_{ji})\n",
    "$$\n",
    "where $f_k$ is the conditional density defined above and $f_{k^*} = \\int f(x_{ji} | \\phi) h(\\phi) d\\phi$ is the prior density of $x_{ji}$, found by integrating out all possible atoms $\\phi_k$.\n",
    "\n",
    "For efficiency, the Gibbs scheme implemented below only samples the $t$ and $k$ indexes (which can later be reverse-engineered to obtain the actual parameters).  The state space of the $k$ values is technically infinite, and the number of tables/dishes currently associated with the data is undefined.  We keep a running list of active $t$ and $k$ values.  Each update step, each customer is assigned either to one of the existing tables or to a new table, and if a customer is assigned to a new table, a new $k$ corresponding value gets drawn; similarly, each table is assigned a dish, either from the existing dishes or with a new dish.  If a table/dish becomes unrepresented in the current scheme, it gets removed from its respective list.  The update full conditionals are:\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "p(t_{ji} = t | t^{-ji}, k, ...) &\\propto \\begin{cases}\n",
    "    n_{jt\\cdot}^{-ji} f_{k_{jt}}^{-x_{ji}}(x_{ji}) & t\\text{ used}\\\\\n",
    "    \\alpha_0 p(x_{ji} | ...) & t\\text{ new}\n",
    "    \\end{cases} \\\\\n",
    "p(k_{jt} = k | t, k^{-jt}) &\\propto \\begin{cases}\n",
    "    m_{\\cdot k} f_k^{-\\mathbf{x}_{jt}}(\\mathbf{x}_{jt}) & k\\text{ used}\\\\\n",
    "    \\gamma f_{k^*}^{-\\mathbf{x}_{jt}}(\\mathbf{x}_{jt}) & k\\text{ new}\n",
    "    \\end{cases} \\\\\n",
    "\\end{aligned} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import digamma\n",
    "\n",
    "def pois_fk(x, k, Kmax, ha, hb):\n",
    "    \"\"\"\n",
    "    Computes in one sweep the mixture components f_k(x_ji) for each x and each k.\n",
    "    MODEL: base measure H ~ Gamma(ha, hb), F(x|phi) ~ Poisson(phi)\n",
    "    All components are calculated exactly in log-space and then exponentiated.\n",
    "    \n",
    "    returns: (N, Kmax) matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x.flatten()  # reshape to 1D, since gibbs routine passes in a 2D array\n",
    "    log_fk_vals = np.zeros((len(k), Kmax))\n",
    "    for kk in range(Kmax):\n",
    "        x_kk = x[k == kk]               # subset of x values with value kk\n",
    "        x_in = (k == kk).astype('int')  # offset for x values in subset\n",
    "        # If a value for k is not used, all mixture components will be 0\n",
    "        if len(x_kk) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Compute (a,b) params from gamma kernel tricks done in fk function\n",
    "        a_denom = np.sum(x_kk) + (len(x_kk) - x_in) * (ha - 1) + 1\n",
    "        b_denom = np.maximum((len(x_kk) - x_in) * (hb + 1), .001)   # if this is 0, it's a problem\n",
    "        a_numer = x + a_denom\n",
    "        b_numer = 1 + b_denom\n",
    "        #print(f\"kk = {kk}, subset size = {len(x_kk)}\")\n",
    "        #print(f\"{np.c_[x,k,a_numer,a_denom,b_numer,b_denom]}\")\n",
    "        log_fk_vals[:, kk] = (-digamma(x + 1) - digamma(a_numer) + digamma(a_denom) +\n",
    "                              a_numer * np.log(b_numer) - a_denom * np.log(b_denom) )\n",
    "    \n",
    "    return log_fk_vals        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFRP:\n",
    "    \"\"\"\n",
    "    Model implementing the Chinese Franchise Restaurant Process.\n",
    "    \n",
    "    CONSTRUCTOR PARAMETERS\n",
    "    - gamma, alpha0: scaling parameters > 0 for base measures H and G0\n",
    "    - f: string representing distribution of data; h is chosen to be conjugate\n",
    "    - hypers: tuple of hyperparameter values specific to f/h scheme chosen\n",
    "    \n",
    "    PRIVATE ATTRIBUTES (volatile)\n",
    "    - t_: set of active t values; formatted as {j: set(t...), ...}\n",
    "    - k_: set of active k values\n",
    "    - tk_: (J x Tmax) of corresponding k values for each t\n",
    "    - n_: (J x Tmax) tensor specifying counts of customers\n",
    "    - m_: (J x Kmax) matrix specifying counts of tables\n",
    "    \n",
    "    PUBLIC ATTRIBUTES\n",
    "    post_samples: (S x 4) matrix of (j, t, k, phi) values for each data point i;\n",
    "                  exists only after gibbs() has been called\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=1, alpha0=1, f='poisson', hypers=None):\n",
    "        self.g_ = gamma\n",
    "        self.a0_ = alpha0\n",
    "        self.set_priors(f, hypers)\n",
    "        \n",
    "    def set_priors(self, f, hypers):\n",
    "        \"\"\"Initializes the type of base measure and data-generation function.\"\"\"\n",
    "        if f == 'poisson':\n",
    "            # Specify parameters of H ~ Gamma(a,b)\n",
    "            if hypers is None:\n",
    "                self.hypers_ = (1,1)\n",
    "            else: self.hypers_ = hypers\n",
    "        self.f_ = f\n",
    "        \n",
    "        \n",
    "    def gibbs(self, x, j, iters=1, Kmax=10):\n",
    "        \"\"\"\n",
    "        Runs the Gibbs sampler to generate posterior estimates of t and k.\n",
    "        x: data matrix, stored row-wise if multidimensional\n",
    "        j: vector of group labels; must have same #rows as x\n",
    "        iters: number of iterations to run\n",
    "        Kmax: maximum number of atoms to draw from base measure H\n",
    "        \n",
    "        results: creation of post_samples attribute\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set the distribution-specific function for fk (conditional density)\n",
    "        fk_routine = None\n",
    "        if self.f_ == 'poisson':\n",
    "            fk_routine = pois_fk;\n",
    "            \n",
    "        group_counts = pd.Series(j).value_counts()\n",
    "        # number of tables cannot exceed size of max group\n",
    "        J, Tmax, N = np.max(j), np.max(group_counts), len(j)\n",
    "        self.n_ = np.zeros((J, Tmax))\n",
    "        self.m_ = np.zeros((J, Kmax))\n",
    "        self.post_samples = np.zeros((iters+1, N, 4), dtype='int')\n",
    "        self.post_samples[:,:,0] = j\n",
    "        self.t_, self.k_ = {}, set()\n",
    "        self.tk_ = np.zeros((J, Tmax))\n",
    "        \n",
    "        # Set random initial values for t and k assignments\n",
    "        t0, k0 = self.post_samples[0,:,1], self.post_samples[0,:,2]   # define two views\n",
    "        t0[:] = np.random.randint(1, Tmax, size=N)\n",
    "        self.t_ = {jj: set(t0[j == jj]) for jj in range(J)}\n",
    "        self.tk_ = np.random.randint(1, Kmax//2, (J, Tmax))           # one table => one dish\n",
    "        for jj in range(J):\n",
    "            for tt in self.t_[jj]:\n",
    "                k0[np.logical_and(j == jj, t0 == tt)] = self.tk_[jj, tt]\n",
    "        \n",
    "        for s in range(iters):\n",
    "            t_prev, k_prev = self.post_samples[s,:,1], self.post_samples[s,:,2]\n",
    "            t_next, k_next = self.post_samples[s+1,:,1], self.post_samples[s+1,:,2]\n",
    "            \n",
    "            # Get a matrix of log fk(x_ji) values (dependent on model specification)\n",
    "            log_fk = fk_routine(x, k_prev, Kmax, *self.hypers_) \n",
    "            self.fk_ = log_fk\n",
    "            \n",
    "            t_next = t_prev\n",
    "            # Cycle through each t value of each customer, conditioning on everything\n",
    "            # Randomize the order in which updates occur\n",
    "            for i in np.random.permutation(N):\n",
    "                continue\n",
    "                \n",
    "            k_next = k_prev\n",
    "            # Similarly, cycle through the k values of each table\n",
    "            for i in np.random.permutation(N):\n",
    "                continue            \n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated data\n",
    "N = 25\n",
    "np.random.seed(3)\n",
    "j = np.random.randint(1, 10, N)\n",
    "x = np.random.poisson(j, N)\n",
    "data = np.c_[x, j]\n",
    "\n",
    "c = CFRP(hypers=(1,10)).gibbs(x[:,None], j)\n",
    "#print(c.post_samples[0,:,:])\n",
    "#print(c.fk_.astype(\"int\"))\n",
    "#print(np.c_[c.post_samples[0,:,2], np.argmax(c.fk_, axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *( 5.2 )* Posterior Sampling with Augmented Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *( 5.3 )* Posterior Sampling by Direct Assignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
