{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import string\n",
    "from itertools import compress\n",
    "from nltk.corpus import stopwords \n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import os\n",
    "from bs4 import BeautifulSoup,SoupStrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.fsencode('data')\n",
    "docs = []\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    f = open(f'data/{filename}', 'r')\n",
    "    data= f.read()\n",
    "    soup = BeautifulSoup(data)\n",
    "    contents = soup.findAll('text')\n",
    "    f.close()\n",
    "    docs.append(str(contents).split('</text>'))\n",
    "\n",
    "docs = [i for doc in docs for i in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split on </dateline> and keep everything after it\n",
    "docs = list(compress(docs, ['</dateline>' in i for i in docs]))\n",
    "docs = [i.split('</dateline>')[1] for i in docs]\n",
    "docs = [i.lower().translate(str.maketrans('\\n', ' ')) for i in docs]\n",
    "docs = [i.translate(str.maketrans('\\r', ' ')) for i in docs]\n",
    "docs = [i.translate(str.maketrans('\\x03', ' ')) for i in docs]\n",
    "docs = [i.translate(str.maketrans('', '', string.punctuation)) for i in docs]\n",
    "docs = [i.translate(str.maketrans('', '', string.digits)) for i in docs]\n",
    "docs = [i.replace('said',' ') for i in docs] # another stop word\n",
    "docs = [i.replace('reuter', ' ') for i in docs]\n",
    "docs = [i.split() for i in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducedVocab(lists, stop_words = None, min_word_count = 10):\n",
    "    '''This function takes a list of words in a list of documents and returns the lists of lists with a reduced\n",
    "       vocabulary, the flattened list, and the vocabulary'''\n",
    "    \n",
    "    if stop_words == None:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stop words\n",
    "    words = [i for sublist in lists for i in sublist if not i in stop_words]\n",
    "\n",
    "    # Remove words that appear less than min_word_count times\n",
    "    wordSeries = pd.Series(words)\n",
    "    vocab = list(compress(wordSeries.value_counts().index, wordSeries.value_counts() >= min_word_count))\n",
    "    \n",
    "    # Recreate lists with filtered vocab\n",
    "    docs = []\n",
    "    for j in range(len(lists)):\n",
    "        docs.append([i for i in lists[j] if i in vocab])\n",
    "    \n",
    "    #flatten docs\n",
    "    one_list = [i for sublist in docs for i in sublist]\n",
    "    \n",
    "    return docs, one_list, vocab\n",
    "\n",
    "def listsToVec(lists, stop_words = None, min_word_count = 10, verbose = 1):\n",
    "    '''\n",
    "    This function takes a list of lists of the words in each document. It removes any stop words, removes words that\n",
    "    appear 'min_word_count' times or less, and maps each word in the documents' vocabulary to a number. \n",
    "    Two flattened vectors are returned, the mapped numbers 'x', and the corresponding document each word belongs to 'j'.'''\n",
    "\n",
    "    # Remove stop words and words that appear less than 'min_word_count' times\n",
    "    docs, one_list, vocab = reducedVocab(lists, stop_words, min_word_count)\n",
    "    \n",
    "    # Map each word to a number\n",
    "    #numbers = list(range(len(vocab)))\n",
    "    #vocab_dict = dict(zip(vocab, numbers))\n",
    "    #x = list(map(vocab_dict.get, one_list))\n",
    "    \n",
    "    # Check for empty lists and print warning if one is found\n",
    "    counter = 0\n",
    "    for i in range(len(docs)-1 ,-1, -1):\n",
    "        if len(docs[i]) == 0:\n",
    "            if verbose > 1:\n",
    "                print(f'WARNING: Document {i} is empty and being removed...')\n",
    "            del docs[i]\n",
    "            counter += 1\n",
    "    \n",
    "    if verbose == 1 and counter > 1:\n",
    "        print(f'WARNING: {counter} documents are empty and being removed...')\n",
    "    \n",
    "    elif verbose == 1 and counter == 1:\n",
    "        print(f'WARNING: {counter} document is empty and being removed...')\n",
    "    \n",
    "    X_matrix = pd.DataFrame(np.zeros((len(one_list), len(vocab))),\n",
    "                           columns=vocab)\n",
    "\n",
    "    for i, word in enumerate(one_list):\n",
    "        X_matrix.loc[i, word] = 1   \n",
    "    \n",
    "    # Determine which document each word belongs to\n",
    "    count, j = 0, []\n",
    "    for i in docs:\n",
    "        j.append([count]*len(i))\n",
    "        count += 1\n",
    "        \n",
    "    # Reduce to a flattened list\n",
    "    j = [i for sublist in j for i in sublist]\n",
    "    \n",
    "    return X_matrix, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Select random subset of the documents\n",
    "selected = np.random.choice(len(docs), 110, replace = False)\n",
    "subset_docs = [docs[i] for i in selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process subset of documents\n",
    "docs, one_list, vocab = reducedVocab(subset_docs, min_word_count = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from pprint import pprint\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_perplexity(docs, max_train_index, min_topics, max_topics):\n",
    "    '''\n",
    "    This function takes a list of lists (words in documents) and the number of documents you want in the training set.\n",
    "    NOTE : max_train_index must be less than len(docs)\n",
    "    \n",
    "    It calculates the perplexity of the trained LDA model on the remaining unseen documents in 'docs' ranging\n",
    "    from n_topics = min_topics to n_topics = max_topics\n",
    "    '''\n",
    "    \n",
    "    assert max_train_index < len(docs), 'max_train_index must be less than the length of docs'\n",
    "    assert min_topics <= max_topics, 'min_topics must be less than or equal to max_topics'\n",
    "    \n",
    "    train = docs[:max_train_index]\n",
    "    test = docs[max_train_index:]\n",
    "    \n",
    "    train_2word = corpora.Dictionary(train)\n",
    "    train_corpus = [train_2word.doc2bow(doc) for doc in train]\n",
    "    \n",
    "    test_2word = corporta.Dictionary(test)\n",
    "    test_corpus = [test_2word.doc2bow(doc) for doc in test]\n",
    "    \n",
    "    perplexity = []\n",
    "    for i in range(min_topics, max_topics + 1): \n",
    "        \n",
    "        lda_model = gensim.models.ldamodel.LdaModel(corpus = train_corpus,\n",
    "                                               id2word = train_2word,\n",
    "                                               num_topics = i,\n",
    "                                               random_state = 23,\n",
    "                                               eval_every = 20,\n",
    "                                               alpha = 'asymmetric',\n",
    "                                               iterations = 500)\n",
    "        \n",
    "        tmp_perplex = np.exp(lda_model.log_perplexity(test_corpus))\n",
    "        perplexity.append(tmp_perplex)\n",
    "        \n",
    "    return perplexity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(docs[:100])\n",
    "corpus = [id2word.doc2bow(text) for text in docs[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=50, \n",
    "                                           random_state=23,\n",
    "                                           eval_every = 20,\n",
    "                                           alpha='asymmetric', # 1D array of length equal to number of expected topics - expresses a-priori belief for each topics prob\n",
    "                                           #eta = , # a-prior belief on word probability\n",
    "                                           iterations = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(docs[100:])\n",
    "test_corpus = [id2word.doc2bow(text) for text in docs[100:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1307833786640772e-09"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(lda_model.log_perplexity(test_corpus))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
