{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import string\n",
    "from itertools import compress\n",
    "from nltk.corpus import stopwords \n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/tdhopper/topic-modeling-datasets/master/data/raw/Nematode%20biology%20abstracts/cgcbib.txt'\n",
    "file = urllib.request.urlopen(url)\n",
    "data = file.read().decode(\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docsToList(data):\n",
    "    '''This function takes a string of abstracts and converts it to a list of lists of the words in each abstract.\n",
    "       This function was made specifically for the data obtained here:\n",
    "       https://raw.githubusercontent.com/tdhopper/topic-modeling-datasets/master/data/raw/Nematode%20biology%20abstracts/cgcbib.txt'''\n",
    "    \n",
    "    # Remove '\\n' and '\\r'\n",
    "    data = data.lower().translate(str.maketrans('\\n', ' '))\n",
    "    data = data.translate(str.maketrans('\\r', ' '))\n",
    "    \n",
    "    # Remove punctuation except for '-' so we can split after each abstract\n",
    "    data = data.translate(str.maketrans('', '', '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))\n",
    "    \n",
    "    # Remove numbers\n",
    "    data = data.translate(str.maketrans('','', string.digits))\n",
    "    \n",
    "    # Split after 'abstract' is stated\n",
    "    data = data.split('-------------------')\n",
    "    # Remove '-' punctuation now\n",
    "    data = [abstract.translate(str.maketrans('-', ' ')) for abstract in data]\n",
    "    \n",
    "    # Remove entries without the word \"abstract\" in it\n",
    "    abs_check = ['abstract' in i for i in data]\n",
    "    data = list(compress(data, abs_check))\n",
    "\n",
    "    # Only keep the words after 'abstract'\n",
    "    data = [abstract.split('abstract')[1] for abstract in data]\n",
    "    \n",
    "    # Remove abstracts that only state 'in french'\n",
    "    not_french = ['in french' not in i for i in data]\n",
    "    data = list(compress(data, not_french))\n",
    "    \n",
    "    # Create list of lists output\n",
    "    output = [i.split() for i in data]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducedVocab(lists, stop_words = None, min_word_count = 10):\n",
    "    '''This function takes a list of words in a list of documents and returns the lists of lists with a reduced\n",
    "       vocabulary, the flattened list, and the vocabulary'''\n",
    "    \n",
    "    if stop_words == None:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stop words\n",
    "    words = [i for sublist in lists for i in sublist if not i in stop_words]\n",
    "\n",
    "    # Remove words that appear less than min_word_count times\n",
    "    wordSeries = pd.Series(words)\n",
    "    vocab = list(compress(wordSeries.value_counts().index, wordSeries.value_counts() >= min_word_count))\n",
    "    \n",
    "    docs = []\n",
    "    for j in range(len(lists)):\n",
    "        docs.append([i for i in lists[j] if i in vocab])\n",
    "    \n",
    "    one_list = reduce(lambda x, y: x + y, docs, [])\n",
    "    return docs, one_list, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listsToVec(lists, stop_words = None, min_word_count = 10):\n",
    "    '''This function takes a list of lists of the words in each document. It removes any stop words, removes words that\n",
    "       appear 10 times or less, and maps each word in the documents' vocabulary to a number. Two flattened vectors are\n",
    "       returned, the mapped numbers 'x', and the corresponding document each word belongs to 'j'.'''\n",
    "\n",
    "    # Remove stop words and words that appear less than 'min_word_count' times\n",
    "    docs, one_list, vocab = reducedVocab(lists, stop_words, min_word_count)\n",
    "    \n",
    "    # Map each word to a number\n",
    "    numbers = list(range(len(vocab)))\n",
    "    vocab_dict = dict(zip(vocab, numbers))\n",
    "    x = list(map(vocab_dict.get, one_list))\n",
    "    \n",
    "    # Check for empty lists and print warning if one is found\n",
    "    for i in range(len(docs)-1 ,-1, -1):\n",
    "        if len(docs[i]) == 0:\n",
    "            print(f'WARNING: Document {i} is empty and being removed...')\n",
    "            del docs[i]\n",
    "    \n",
    "    # Determine which document each word belongs to\n",
    "    count, j = 0, []\n",
    "    for i in docs:\n",
    "        j.append([count]*len(i))\n",
    "        count += 1\n",
    "        \n",
    "    # Reduce to a flattened list\n",
    "    j = reduce(lambda x, y: x + y, j, [])\n",
    "    \n",
    "    return x,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = docsToList(data)\n",
    "x, j = listsToVec(lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morris/Quinn\n",
    "\n",
    "What is the etiquette for using other packages inside my functions? Is there something I need to do to ensure the person has them installed?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
