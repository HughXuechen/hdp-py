{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import string\n",
    "from itertools import compress\n",
    "from nltk.corpus import stopwords \n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/tdhopper/topic-modeling-datasets/master/data/raw/Nematode%20biology%20abstracts/cgcbib.txt'\n",
    "file = urllib.request.urlopen(url)\n",
    "data = file.read().decode(\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docsToList(data):\n",
    "    '''This function takes a string of abstracts and converts it to a list of lists of the words in each abstract.\n",
    "       This function was made specifically for the data obtained here:\n",
    "       https://raw.githubusercontent.com/tdhopper/topic-modeling-datasets/master/data/raw/Nematode%20biology%20abstracts/cgcbib.txt'''\n",
    "    \n",
    "    # Remove '\\n' and '\\r'\n",
    "    data = data.lower().translate(str.maketrans('\\n', ' '))\n",
    "    data = data.translate(str.maketrans('\\r', ' '))\n",
    "    \n",
    "    # Remove punctuation except for '-' so we can split after each abstract\n",
    "    data = data.translate(str.maketrans('', '', '!\"#$%&\\'()*+,./;<=>?@[\\\\]^_`{|}~'))\n",
    "    \n",
    "    # Remove numbers\n",
    "    data = data.translate(str.maketrans('','', string.digits))\n",
    "    \n",
    "    # Split after 'abstract' is stated\n",
    "    data = data.split('-------------------')\n",
    "    # Remove '-' punctuation now\n",
    "    data = [abstract.translate(str.maketrans('-', ' ')) for abstract in data]\n",
    "    \n",
    "    # Remove entries without the word \"abstract\" in it\n",
    "    abs_check = ['abstract' in i for i in data]\n",
    "    data = list(compress(data, abs_check))\n",
    "\n",
    "    # Only keep the words after 'abstract'\n",
    "    data = [abstract.split('abstract:')[1] for abstract in data]\n",
    "    \n",
    "    # Remove :\n",
    "    data = [abstract.translate(str.maketrans(':', ' ')) for abstract in data]\n",
    "    \n",
    "    # Remove abstracts that only state 'in french'\n",
    "    not_french = ['in french' not in i for i in data]\n",
    "    data = list(compress(data, not_french))\n",
    "    \n",
    "    # Create list of lists output\n",
    "    output = [i.split() for i in data]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducedVocab(lists, stop_words = None, min_word_count = 10):\n",
    "    '''This function takes a list of words in a list of documents and returns the lists of lists with a reduced\n",
    "       vocabulary, the flattened list, and the vocabulary'''\n",
    "    \n",
    "    if stop_words == None:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stop words\n",
    "    words = [i for sublist in lists for i in sublist if not i in stop_words]\n",
    "\n",
    "    # Remove words that appear less than min_word_count times\n",
    "    wordSeries = pd.Series(words)\n",
    "    vocab = list(compress(wordSeries.value_counts().index, wordSeries.value_counts() >= min_word_count))\n",
    "    \n",
    "    # Recreate lists with filtered vocab\n",
    "    docs = []\n",
    "    for j in range(len(lists)):\n",
    "        docs.append([i for i in lists[j] if i in vocab])\n",
    "    \n",
    "    #flatten docs\n",
    "    one_list = [i for sublist in docs for i in sublist]\n",
    "    \n",
    "    return docs, one_list, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listsToVec(lists, stop_words = None, min_word_count = 10, verbose = 1):\n",
    "    '''This function takes a list of lists of the words in each document. It removes any stop words, removes words that\n",
    "       appear 10 times or less, and maps each word in the documents' vocabulary to a number. Two flattened vectors are\n",
    "       returned, the mapped numbers 'x', and the corresponding document each word belongs to 'j'.'''\n",
    "\n",
    "    # Remove stop words and words that appear less than 'min_word_count' times\n",
    "    docs, one_list, vocab = reducedVocab(lists, stop_words, min_word_count)\n",
    "    \n",
    "    # Map each word to a number\n",
    "    numbers = list(range(len(vocab)))\n",
    "    vocab_dict = dict(zip(vocab, numbers))\n",
    "    x = list(map(vocab_dict.get, one_list))\n",
    "    \n",
    "    # Check for empty lists and print warning if one is found\n",
    "    counter = 0\n",
    "    for i in range(len(docs)-1 ,-1, -1):\n",
    "        if len(docs[i]) == 0:\n",
    "            if verbose > 1:\n",
    "                print(f'WARNING: Document {i} is empty and being removed...')\n",
    "            del docs[i]\n",
    "            counter += 1\n",
    "    \n",
    "    if verbose == 1 and counter > 1:\n",
    "        print(f'WARNING: {counter} documents are empty and being removed...')\n",
    "    \n",
    "    elif verbose == 1 and counter == 1:\n",
    "        print(f'WARNING: {counter} document is empty and being removed...')\n",
    "    \n",
    "    \n",
    "    # Determine which document each word belongs to\n",
    "    count, j = 0, []\n",
    "    for i in docs:\n",
    "        j.append([count]*len(i))\n",
    "        count += 1\n",
    "        \n",
    "    # Reduce to a flattened list\n",
    "    j = [i for sublist in j for i in sublist]\n",
    "    \n",
    "    return x,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: 231 documents are empty and being removed...\n"
     ]
    }
   ],
   "source": [
    "lists = docsToList(data)\n",
    "x, j = listsToVec(lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morris/Quinn\n",
    "\n",
    "What is the etiquette for using other packages inside my functions? Is there something I need to do to ensure the person has them installed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization of reducedVocab funcion\n",
    "\n",
    "Originally the function `reducedVocab` runs on our dataset in Wall time: 3 min 6s\n",
    "\n",
    "Currently the function `reducedVocab` runs in Wall time: 1min 56s\n",
    "\n",
    "Comparatively, gensim's `corpora.Dictionary` runs in 1.17s and the `doc2bow` for text in docs runs in 1.02s.\n",
    "\n",
    "Our function would run in 2s if it did not have to filter the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 201 ms\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "%time words = [i for sublist in lists for i in sublist if not i in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 23s\n",
      "Wall time: 107 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "550318"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Old way (1min 23s)\n",
    "%time j = reduce(lambda x, y: x + y, docs, [])\n",
    "\n",
    "# New way (107ms)\n",
    "%time [i for sublist in docs for i in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 76.2 ms\n",
      "Wall time: 510 ms\n"
     ]
    }
   ],
   "source": [
    "%time wordSeries = pd.Series(words)\n",
    "%time vocab = list(compress(wordSeries.value_counts().index, wordSeries.value_counts() >= 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "count, j = 0, []\n",
    "docs = []\n",
    "\n",
    "# Old way (2min 7s)\n",
    "%time for j in range(len(lists)): docs.append([i for i in lists[j] if i in vocab])\n",
    "    \n",
    "# New way... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from pprint import pprint\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%time docs, one_list, vocab = reducedVocab(lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%time id2word = corpora.Dictionary(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.02 s\n"
     ]
    }
   ],
   "source": [
    "%time corpus = [id2word.doc2bow(text) for text in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=23,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='asymmetric', # 1D array of length equal to number of expected topics - expresses a-priori belief for each topics prob\n",
    "                                           #eta = , # a-prior belief on word probability\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has lots of problems... \n",
    "- The log perplexity method doesn't return 'perplexity', need to calculate that on my own. \n",
    "- Need to implement 10-fold cross-validation\n",
    "- Mixture component cardinalities ranging from 10 to 120\n",
    "- Need to figure out how to incorporate the symmetric Dirichlet distribution with parameters \n",
    "    of .5 for the prior H over topic distributions\n",
    "- Distribution over topics in LDA was assumed to be symmetric Dirichlet w \n",
    "    parameters $\\alpha_0/L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
